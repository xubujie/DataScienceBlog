<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://xubujie.github.io/DataScienceBlog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xubujie.github.io/DataScienceBlog/" rel="alternate" type="text/html" /><updated>2020-08-09T09:00:14-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/feed.xml</id><title type="html">Jay’s Blog</title><subtitle>Sharing my learning and idea</subtitle><entry><title type="html">Bert Pre-training of Deep Bidirectional Transformers for Language Understanding</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/08/09/Bert.html" rel="alternate" type="text/html" title="Bert Pre-training of Deep Bidirectional Transformers for Language Understanding" /><published>2020-08-09T00:00:00-05:00</published><updated>2020-08-09T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/08/09/Bert</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/08/09/Bert.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-09-Bert.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-&amp;#31616;&amp;#20171;&quot;&gt;1. &amp;#31616;&amp;#20171;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-&amp;#31616;&amp;#20171;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在2014年Google发表attention机制以来，attention得到了广泛的研究，2017年Google又在“Attention Is All You Need”这篇论文里提出了Transformer模型。在对序列问题的建模上，Transformer大有取代RNN地位的势头。随后，在2019年Bert横空出世，横扫几乎所有NLP任务的榜单，引爆了NLP的研究社区。让越来越多的人投入到了这个领域的研究上来了。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文简要介绍提出BERT的那篇论文。将从以下几个方面展开：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;背景和相关研究&lt;/li&gt;
&lt;li&gt;BERT的结构和任务&lt;/li&gt;
&lt;li&gt;BERT在NLP任务中的应用&lt;/li&gt;
&lt;li&gt;总结和反思&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在阅读这篇文章之前，读者最好对Transformer的模型结构有所了解。可以参考&lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;The Illustrated Transformer&lt;/a&gt;(这算是介绍Transformer最好的文章了)。如果想要了解Transformer的实现，可以阅读&lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt;。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-&amp;#32972;&amp;#26223;&amp;#21644;&amp;#30456;&amp;#20851;&amp;#30740;&amp;#31350;&quot;&gt;2. &amp;#32972;&amp;#26223;&amp;#21644;&amp;#30456;&amp;#20851;&amp;#30740;&amp;#31350;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-&amp;#32972;&amp;#26223;&amp;#21644;&amp;#30456;&amp;#20851;&amp;#30740;&amp;#31350;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在很多NLP的任务中，使用预训练模型对结果都有很大的提高。例如，在NLP任务中使用预训练的词向量，能够很好的提高模型精度。但是传统的词向量表征存在一个致命的问题，就是他们无法有效的表达一词多义的情况。随后ELMo, GPT, BERT相继得到提出，他们都可以很好的解决一词多义的问题。Elmo用两层LSTM独立的训练了从左到右和从右到左的语言表示，最终得到下游任务的特征表示，对不同任务，他需要训练不同的特征表示。GPT使用了Transformer从左到右对语料进行训练。BERT采用了双向Transformer对语料进行了训练。BERT和GPT都可以直接用于下游任务，并通过fine-tune微调网络参数。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Bert1.PNG&quot; alt=&quot;bert1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Bert&amp;#30340;&amp;#32467;&amp;#26500;&amp;#21644;&amp;#20219;&amp;#21153;&quot;&gt;3. Bert&amp;#30340;&amp;#32467;&amp;#26500;&amp;#21644;&amp;#20219;&amp;#21153;&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Bert&amp;#30340;&amp;#32467;&amp;#26500;&amp;#21644;&amp;#20219;&amp;#21153;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;如上图所示，BERT是由多层的双向Transformer构成。在结构上，他相较于GPT的创新就是引入了双向Transformer。另外，BERT还引入了Masked Language Model(MLM)和Next Sentence Prediction(NSP)两个任务来做模型的训练。以下分别对他们进行介绍。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Masked-LM&quot;&gt;Masked LM&lt;a class=&quot;anchor-link&quot; href=&quot;#Masked-LM&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;为了使用双向Transformer，Bert里设计了一种mask机制，就是对输入进行mask操作，然后预测被mask的词。具体的mask操作如下,对于选中的单词&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;80%的概率置换成[MASK]&lt;/li&gt;
&lt;li&gt;10%的概率换成一个随机的单词&lt;/li&gt;
&lt;li&gt;10%的概率保留原单词&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面给出了这个任务的一个例子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Next-Sentence-Prediction&quot;&gt;Next Sentence Prediction&lt;a class=&quot;anchor-link&quot; href=&quot;#Next-Sentence-Prediction&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在很多NLP的任务里，理解句子间的关系很重要。比如说QA任务，或者是NLI任务。所以，在BERT中作者设计了NSP这个任务，这个任务其实很简单，就是去判断一句话是否跟另一句话相连。下面给出了两个例子。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;pre&gt;&lt;code&gt;Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence

Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;通过研究表明，NSP这个任务对于QA任务或者是NLI任务都有很大的帮助。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-Bert&amp;#22312;NLP&amp;#20219;&amp;#21153;&amp;#20013;&amp;#30340;&amp;#24212;&amp;#29992;&quot;&gt;4. Bert&amp;#22312;NLP&amp;#20219;&amp;#21153;&amp;#20013;&amp;#30340;&amp;#24212;&amp;#29992;&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Bert&amp;#22312;NLP&amp;#20219;&amp;#21153;&amp;#20013;&amp;#30340;&amp;#24212;&amp;#29992;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在训练完BERT后，只用稍微进行以下Fine-tune，即可运用于其他的下游任务。
&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Bert2.PNG&quot; alt=&quot;Bert Pretrain&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;下图展示了如何在各个不同NLP任务中使用BERT。业界戏称此图为麻将图&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Bert4.PNG&quot; alt=&quot;Bert4&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本篇论文的结果是基于$BERT_{BASE}$和$BERT_{LARGE}$的，他们的参数如下(L为Transformer的层数，H为hidden size, A为self-attention的头的个数:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$BERT_{BASE}$: L=12, H=768, A=12&lt;/li&gt;
&lt;li&gt;$BERT_{LARGE}$: L=24, H=1024, A=16&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;BERT横扫了多少NLP的下游任务版单，在这里我们就不一一列举了。下表列出了BERT在GLUE这个Benchmark上的表现，可以看出它对当时的SOTA有很大的提升。另外在SQuAD, SWAG等任务或者数据集上，BERT也是轻松刷新了最好成绩。
&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Bert_Table1.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;5.-&amp;#24635;&amp;#32467;&amp;#21644;&amp;#21453;&amp;#24605;&quot;&gt;5. &amp;#24635;&amp;#32467;&amp;#21644;&amp;#21453;&amp;#24605;&lt;a class=&quot;anchor-link&quot; href=&quot;#5.-&amp;#24635;&amp;#32467;&amp;#21644;&amp;#21453;&amp;#24605;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;BERT的提出对于NLP领域有着至关重要的影响，在这之后Transfer Learning在NLP领域也渐渐得到广泛的应用。正如作者所说，BERT的模型结构不复杂，但是在实用中非常强大。Google能够在AI领域屡屡发表重要文章，理论是一个方面，代码能力也非常重要。能想到BERT结构的人应该不少，但是能把它实现起来确实比较难。&lt;/p&gt;
&lt;p&gt;另外，虽然NSP用于训练BERT对于一些NLP的下游任务有帮助，不过另外一些任务，例如文本分类。可能NSP的帮助不是很大，这里应该有提升的空间。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt;&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. &lt;a href=&quot;https://github.com/google-research/bert&quot;&gt;BERT&lt;/a&gt;&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">基于类别的情感分析系统搭建</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/08/07/Sentiment-Analysis-System.html" rel="alternate" type="text/html" title="基于类别的情感分析系统搭建" /><published>2020-08-07T00:00:00-05:00</published><updated>2020-08-07T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/08/07/Sentiment-Analysis-System</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/08/07/Sentiment-Analysis-System.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-07-Sentiment-Analysis-System.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-&amp;#39033;&amp;#30446;&amp;#32972;&amp;#26223;&amp;#21644;&amp;#38382;&amp;#39064;&amp;#25551;&amp;#36848;&quot;&gt;1. &amp;#39033;&amp;#30446;&amp;#32972;&amp;#26223;&amp;#21644;&amp;#38382;&amp;#39064;&amp;#25551;&amp;#36848;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-&amp;#39033;&amp;#30446;&amp;#32972;&amp;#26223;&amp;#21644;&amp;#38382;&amp;#39064;&amp;#25551;&amp;#36848;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;维基百科的定义里说，文本情感分析是指用自然语言处理、文本挖掘以及计算机语言学等方法来识别和提取原素材中的主观信息。情感分析的目的是找出作者对某个事物的观点。很多人在网上购物或者在外就餐之后，都会通过发表评论来描述自己的体验。对于商家来说，分析这些文本就可以较好的看清楚用户的反馈，从而提升服务质量。另外，在投资领域，情感分析也有所应用。通过分析twitter上对某家公司的整体情感，可以给出大众对这家公司的一个整体看法，从而对投资起到一个指导作用。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文将介绍怎么样从0开始搭建一个简单的情感分析系统。所用的到的数据来自&lt;a href=&quot;https://www.yelp.com/dataset/documentation/main&quot;&gt;Yelp Dataset&lt;/a&gt;, Yelp对标国内的大众点评，这个数据集里收集了很多的用户评论信息。下图是来自yelp网站的一个例子。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/sentiment1.PNG&quot; alt=&quot;yelp&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在Review Highlights里，我们可以看到一些用户评价，而且Yelp还对评价进行了归类，比如第一类是有对souvenirs的评价，第二类是对electronics的评价。这里的souvenirs和electronics我们可以看作是商品或者是服务的一个维度(Aspect)。我们要搭建的情感分析系统要在传统的情感分析系统上做一点提高。&lt;/p&gt;
&lt;p&gt;我们不断要判断用户评价是否是积极的，而且我们要从用户评价中获取用户是对哪个商品维度所进行的描述，并判断用户对每一个商品维度的情感。
对于每一条评价，我们想通过系统获取如下结果。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{
    Business Name: XXXXX, 
    Overall Rating: X,
    aspect1: { rating: XXX, pos: [XXX], neg: [XXX]}
    aspect2: {rating: XXX, pos: [XXX], neg: [XXX]}
    aspect3: {rating: XXX, pos: [XXX], neg: [XXX]} ……
    aspect5: {rating: XXX, pos:[xxx], neg:[xxxx]}
}&lt;/code&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-baseline&amp;#25645;&amp;#24314;&quot;&gt;2. baseline&amp;#25645;&amp;#24314;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-baseline&amp;#25645;&amp;#24314;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在这里，我们用到yelp数据集里的“yelp_academic_dataset_business”和“yelp_academic_dataset_review”。可以通过这个链接查看&lt;a href=&quot;https://www.yelp.com/dataset/documentation/main&quot;&gt;样本数据&lt;/a&gt;。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;我们的baseline包含以下几个部分&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;获取每个business的overall Rating&lt;/li&gt;
&lt;li&gt;获取每一个business的aspect&lt;/li&gt;
&lt;li&gt;获取对于aspect的评价&lt;ol&gt;
&lt;li&gt;对于每个aspect的综合评价&lt;/li&gt;
&lt;li&gt;从用户评论中获得对于某个aspect的相关内容，并进行分类（判断是好评还是差评）&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;&amp;#33719;&amp;#21462;Overall-Rating&quot;&gt;&amp;#33719;&amp;#21462;Overall Rating&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#33719;&amp;#21462;Overall-Rating&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;我们只需要对每个business的所有评论的结果求平均即可得到Overall Rating&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Id&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;business_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;star&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reviews&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;star&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;&amp;#33719;&amp;#21462;&amp;#27599;&amp;#19968;&amp;#20010;business&amp;#30340;aspect&quot;&gt;&amp;#33719;&amp;#21462;&amp;#27599;&amp;#19968;&amp;#20010;business&amp;#30340;aspect&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#33719;&amp;#21462;&amp;#27599;&amp;#19968;&amp;#20010;business&amp;#30340;aspect&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;提取aspect，我们采用最简单的方法，对于每一个business，循环所有评论，统计评论中的名字词频，在去除stopwords之后，找到排名前5的词语，即作为该business的top 5 aspects。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# 从每个评论中提取名词&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;extract_and_add_aspects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspects_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;business_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;business_id&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aspects&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nltk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos_tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tagset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;universal&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;NOUN&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aspects_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;business_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 对于每一个business，提取在评论中词频最高的5个名词（去除stopword之后）&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_top_5_aspects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspects_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;business_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itertools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chain&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_iterable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspects_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;business_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sorted_l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;top_5_aspects&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sorted_l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;top_5_aspects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;top_5_aspects&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_5_aspects&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;&amp;#33719;&amp;#21462;&amp;#23545;&amp;#20110;aspect&amp;#30340;&amp;#35780;&amp;#20215;&quot;&gt;&amp;#33719;&amp;#21462;&amp;#23545;&amp;#20110;aspect&amp;#30340;&amp;#35780;&amp;#20215;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#33719;&amp;#21462;&amp;#23545;&amp;#20110;aspect&amp;#30340;&amp;#35780;&amp;#20215;&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;为了获取对每一个aspect的评价，我们首先需要训练一个情感分析的模型。由于是baseline，我们首先尝试一个简单的模型来建立这个情感分析模型。本文采用tfidf vectorizer+random forest来建模。详细步骤分为以下几步&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对所有的review中的文本（text）进行tfidf变换&lt;/li&gt;
&lt;li&gt;将tfidf生成的矩阵作为输入，review的stars作为输出进行模型的训练。&lt;/li&gt;
&lt;li&gt;用random forest来模型进行训练。因为这里的stars的取值为[0,1,2,3,4,5],　是一个有序的分类问题，我们也可以把该问题看成是一个回归问题，用regressor的效果会更好&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestRegressor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tidif_vectorizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;review_tidif_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tidif_vectorizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;review_tidif_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;训练完模型之后，我们还需要做关键的另一步，就是从用户评论中获取跟对应aspect相关的部分。这里，我们采取最简单的方法，首先按标点符号(这里我们只用了',.!')将用户评价进行切分。然后对于每个aspect，如果分句中出现了他，则判断该分局与这个aspect有关。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_segment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Input an aspect and a text, the function will return the setences contain the aspect,&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    if there are no such aspec the segment could be empty list.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;setences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;[,.!]&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;segment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;setences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspect&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;segment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;segment&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;完成了上两步后，我们既可以对每一个business的每一个review进行分析，求得最终结果。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;business_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_reviews&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contains&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aspect1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;review_segment&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_segment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentiment_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_prob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;review_segment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;pos_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;neg_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;review&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-&amp;#32467;&amp;#26524;&quot;&gt;3. &amp;#32467;&amp;#26524;&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-&amp;#32467;&amp;#26524;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;以下给出一个对于某个business的结果精选。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;pre&gt;&lt;code&gt;========= sample result for business_id jRfQX8enRhWHf7V5zP5U8g =========

Business Name:  The Gelato Spot
Overall Rating:  4.058315334773218

Aspects1 :gelato, rating:3.7201255688450567, 
pos_sample: ['This review is only with respect to their gelato', &quot;We haven't yet ventured beyond gelato but will make it our next stop when we're in the mood for pizza&quot;, 'The gelato LOOKS delicious from the get-go', 'The guy behind the gelato counter was very helpful and friendly', 'I saw a decent amount of customer flow and the employees were constantly making gelato in the back'], 
neg_sample: [&quot;Worst gelato I've had&quot;, 'The girls serving gelato had horrible attitudes and kept pushing me off to each other (they openly said they did not want to wait on me and told each other to just have 
me wait and to help someone else)', 'their gelato was nothing special']

Aspects2 :pizza, rating:3.6547648329047857, 
pos_sample: [&quot;We haven't yet ventured beyond gelato but will make it our next stop when we're in the mood for pizza&quot;, &quot;one of these days I'll try the pizza place as well and I'm sure it won't disappoint&quot;, 'Vegetarian Friendly Entree: Honey Basil Pizza\nDelicious pizza with the sweet and savory combination', 'One pizza is a perfect serving for one person', &quot;I'm the type of 
person that can never choose the type of pizza I want&quot;], 
neg_sample: ['Showed up Saturday night to order a pizza and was told I could order but had to take it to go since they were closing in 30 minutes', 'Was told it took 15 minutes for the pizza to be ready', 'Worst pizza I have ever had', 'Lean cuisine pizzas have more flavor', &quot;the margherite pizza was the worst I've had&quot;]

Aspects3 :place, rating:3.671313509696845, 
pos_sample: ['The place is very clean and nicely decorated', 'This place is great', 'The place is conveniently located in a corner location within a pizzeria', 'The location is clean and the place is quaint and comfortable', 'I frequent this place more often than I care to admit but with free samples and a great variety of different gelato flavors how could I not? Some common reasons why you might find me in Gelato Spot:\n\n- Holiday\n- Bad Day\n- Good Day\n- Normal Day\n- Sunday'], neg_sample: ['I decided to order pick up using the Eat2 services for lunch since my work place is walking distance from the restaurant', 'After helping them I told her I was there to pick up an order i placed using the Yelp app', 'I showed her the charge from my bank account and she placed the order to the cooks', 'I had to rush back to my work place since I only have a 30min lunch']

Aspects4 :flavors, rating:3.6733356958870567, 
pos_sample: ['Great flavors and good quality', 'Their flavors are all delicious', 'and they are great at suggesting new flavors', 'and they have so many flavors to choose from', &quot;We haven't been disappointed in any flavors that we've tried:  tiramisu&quot;], 
neg_sample: []

Aspects5 :spot, rating:3.6346406123098576, 
pos_sample: ['I was pleasantly surprised with this spot', 'It hit the spot', 'I am so happy that we found this spot and if I ever come back', 'This review is for Wood Fired pizza which is connected to the gelato spot', 'The G spot has it all'], 
neg_sample: ['My daughter read about their cakes on their website so when I asked her where I should get her birthday cake she told me that Gelato spot had cakes with buttercream frosting(her favorite)']&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-&amp;#20854;&amp;#20182;&amp;#23581;&amp;#35797;&amp;#21644;&amp;#21453;&amp;#24605;&quot;&gt;4. &amp;#20854;&amp;#20182;&amp;#23581;&amp;#35797;&amp;#21644;&amp;#21453;&amp;#24605;&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-&amp;#20854;&amp;#20182;&amp;#23581;&amp;#35797;&amp;#21644;&amp;#21453;&amp;#24605;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在文本数据的预处理方面，常用的还有count vectorizer和word embedding， count vectorizer的效果比起tfidf略微差了一点，word embedding对结果会有所提高。另外，在情感分析模型的构建中，还可以有很多其他的方法，例如目前最流行的Bert。或者也可以利用LSTM来构建模型。在这方面，如果有兴趣可以参考&lt;a href=&quot;https://www.kaggle.com/poonaml/bidirectional-lstm-spacy-on-yelp-reviews。&quot;&gt;https://www.kaggle.com/poonaml/bidirectional-lstm-spacy-on-yelp-reviews。&lt;/a&gt; 结果会比baseline有很大的提升。不过如果想要尝试这类深度学习的方法，必须先准备好GPU。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;搭建一个情感分析系统需要牵扯到很多方面，任何一个环节都需要注意。如果有一个环节出错，都会影响到整个系统的表现。例如Aspect的抽取，Segment的抽取，或者是模型的构建。对于Aspect的抽取，Segment的抽取。本文的方法比较基础，这是考虑了最基础的情况。这些方面的抽取也可以考虑通过训练模型来实现。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &lt;a href=&quot;https://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90&quot;&gt;文本情感分析&lt;/a&gt;&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &lt;a href=&quot;https://www.kaggle.com/poonaml/bidirectional-lstm-spacy-on-yelp-reviews&quot;&gt;Bidirectional LSTM, SpaCy on Yelp Reviews&lt;/a&gt;&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. &lt;a href=&quot;https://www.yelp.com/dataset/documentation/main&quot;&gt;Yelp Dataset&lt;/a&gt;&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">Sequenc to Sequence Learning with Nueral Networks</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks.html" rel="alternate" type="text/html" title="Sequenc to Sequence Learning with Nueral Networks" /><published>2020-07-31T00:00:00-05:00</published><updated>2020-07-31T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-31-Sequence-to-Sequence-Learning-with-Nueral-Networks.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;这篇文章发表于2014年底。我们所熟知的seq2seq模型由该文章提出。本文提出了一种解决由一个序列预测另一个序列问题的端到端的模型(seq2seq模型)。通过在WMT'14数据集上的测试，证明了端到端的seq2seq模型可以达到SOAT的精度。&lt;/p&gt;
&lt;p&gt;接下来我们主要介绍以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;背景和模型概要&lt;/li&gt;
&lt;li&gt;实验设计和工程实现&lt;/li&gt;
&lt;li&gt;实验结果&lt;/li&gt;
&lt;li&gt;总结&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-&amp;#32972;&amp;#26223;&amp;#21644;&amp;#27169;&amp;#22411;&amp;#27010;&amp;#35201;&quot;&gt;1. &amp;#32972;&amp;#26223;&amp;#21644;&amp;#27169;&amp;#22411;&amp;#27010;&amp;#35201;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-&amp;#32972;&amp;#26223;&amp;#21644;&amp;#27169;&amp;#22411;&amp;#27010;&amp;#35201;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;传统的DNN虽然在很多领域取得了很好的成绩，但是DNN之只能适用于输入和输出的维度都确定的情况下。对于输入和输出长度不固定的问题，传统的DNN束手无策。
在本篇文章，作者为了解决此问题，提出了一种seq2seq的模型来处理。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq.PNG&quot; alt=&quot;seq2seq&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;如上图所示，作者利用一个LSTM来编码输入序列，然后用另一个LSTM来对输出进行解码。这里用&quot;EOS&quot;来表示句尾，从而使得输出的序列长度由模型来决定。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-&amp;#23454;&amp;#39564;&amp;#35774;&amp;#35745;&amp;#21644;&amp;#24037;&amp;#31243;&amp;#23454;&amp;#29616;&quot;&gt;2. &amp;#23454;&amp;#39564;&amp;#35774;&amp;#35745;&amp;#21644;&amp;#24037;&amp;#31243;&amp;#23454;&amp;#29616;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-&amp;#23454;&amp;#39564;&amp;#35774;&amp;#35745;&amp;#21644;&amp;#24037;&amp;#31243;&amp;#23454;&amp;#29616;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.1-&amp;#25968;&amp;#25454;&amp;#21644;&amp;#35780;&amp;#20215;&amp;#25351;&amp;#26631;&quot;&gt;2.1 &amp;#25968;&amp;#25454;&amp;#21644;&amp;#35780;&amp;#20215;&amp;#25351;&amp;#26631;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.1-&amp;#25968;&amp;#25454;&amp;#21644;&amp;#35780;&amp;#20215;&amp;#25351;&amp;#26631;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;实验所采用的数据集来自WMT'14里英文到法文的翻译任务的数据。总共句子数为12M，其中法文单词有348M，英文单词有304M。另外作者事先给定了两种语言的词库，英文词库包含160,000常用词，法文词库包含80,000常用词，其他单词用&quot;UNK&quot;表示。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;BLEU来评价机器翻译的好坏。BLEU公式如下&lt;/p&gt;
$$
BLEU = BP \times e^{\sum_{i=1}^nw_i\log(p_i)}
$$$$
BP = 
\begin{cases}
1 &amp;amp;\text{if c&amp;gt;r} \\
e^{1-r/c} &amp;amp;c \leq r
\end{cases}
$$&lt;p&gt;这里r为一个参考长度，当翻译结果长度为r时，不需要对结果进行惩罚&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.2-&amp;#35757;&amp;#32451;&amp;#35814;&amp;#24773;&quot;&gt;2.2 &amp;#35757;&amp;#32451;&amp;#35814;&amp;#24773;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.2-&amp;#35757;&amp;#32451;&amp;#35814;&amp;#24773;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;模型由4层LSTM作为编码器，另外一个4层LSTM作为解码器。输入的词向量维度为1000。其他实验中的细节如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用一个范围为-0.08到0.08的均一分布来初始化LSTM的参数&lt;/li&gt;
&lt;li&gt;用SGD进行训练，初始5个epoch的学习率设为0.7，之后每半个epoch学习率减半，总共训练了7.5个epoch&lt;/li&gt;
&lt;li&gt;batch size为128， 每个batch里面的句子长度尽量相近&lt;/li&gt;
&lt;li&gt;为了防止梯度爆炸，采取了梯度裁剪&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;此外，作者还提到了一个特别重要的技巧，就是将输入进行倒排。通过实验，发现这样可以大大提升结果。从直观分析来说，这个技巧能起到作用的原因是将输出的词和输出的词在神经网络上的距离拉近了。例如在figure1里面，原来A到X的距离为4个单元，通过倒排A到X的距离变为2个单元。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在工程实现上，作者采用了8个GPU，每一层的LSTM用一个GPU计算，在前一个GPU计算完结果后会传递到下一个GPU来计算。另外4个GPU则用于softmax的并行计算。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-&amp;#23454;&amp;#39564;&amp;#32467;&amp;#26524;&quot;&gt;3. &amp;#23454;&amp;#39564;&amp;#32467;&amp;#26524;&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-&amp;#23454;&amp;#39564;&amp;#32467;&amp;#26524;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;实验结果如下面的table2所示，可以看出端到端的Seq2Seq模型在翻译任务上取到了比较不错的结果。提高Beam search的size和增加集成模型的各种都有助于提高模型精度。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq2.PNG&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;另外，作者还分析了编码器对于语序和语态的表现能力。从图中可以看出模型对于语序非常敏感，但是不能很好的区分主动语态和被动语态。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq3.PNG&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;最后作者比较了LSTM的seq2seq模型和baseline(统计模型)在各种句子长度下的表现，发现LSTM在各种句子长度下都能有比较好的效果。
另外figure3的右图指出，在句子里常用词比较多的情况下，LSTM的表现会更好。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq4.PNG&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-&amp;#24635;&amp;#32467;&quot;&gt;4. &amp;#24635;&amp;#32467;&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-&amp;#24635;&amp;#32467;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文提出了一种处理不固定长序列问题的端到端的方法。大大简化了以往统计建模的时间，并且精度上也达到了一个不错的级别。个人感觉，在建模上，大家可以都能想到seq2seq这种结构。但是一些trick(比如倒排输入的句子)不太容易想到，而且在工程实现上，利用多个GPU并行计算LSTM在当时也不容易做到&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &lt;a href=&quot;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &lt;a href=&quot;https://blog.csdn.net/allocator/article/details/79657792&quot;&gt;自然语言处理——BLEU详解以及简单的代码实现&lt;/a&gt;&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">Evaluation methods for unsupervised word embeddings</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings.html" rel="alternate" type="text/html" title="Evaluation methods for unsupervised word embeddings" /><published>2020-07-04T00:00:00-05:00</published><updated>2020-07-04T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-04-Evaluation-methods-for-unsupervised-word-embeddings.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文比较了各种衡量词向量的方法，并提出了一种新的评测词向量的方法。本文主要有以下贡献&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分析了不同评判标准间的关系，表明了生成词向量的方式要和特殊任务相关联&lt;/li&gt;
&lt;li&gt;提出了一种通过人为评分方式衡量直接衡量单个词向量的方法&lt;/li&gt;
&lt;li&gt;提出了选择词向量（用于评价）时要考虑到选择不同词频，词性，词义的向量。保证数据的多样性&lt;/li&gt;
&lt;li&gt;本文还发现了词向量包含着词频信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是这篇文章的目的不是去比较词向量的好坏，而是去研究评判词向量方法的差别。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-Embeeding&amp;#30340;&amp;#20934;&amp;#22791;&quot;&gt;1. Embeeding&amp;#30340;&amp;#20934;&amp;#22791;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Embeeding&amp;#30340;&amp;#20934;&amp;#22791;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文准备了以下六种生成词向量的方式用于评判：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于概率预测的embeeding&lt;ul&gt;
&lt;li&gt;CBOW model of word2vec (Mikolov et al 2013a)&lt;/li&gt;
&lt;li&gt;C&amp;amp;W embeddings (Collobert et al. 2011)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于反应语料中的词汇的同现关系&lt;ul&gt;
&lt;li&gt;Hellinger PCA (Lebret and COllobert 2014)&lt;/li&gt;
&lt;li&gt;GloVe (Pennington et al., 2014)&lt;/li&gt;
&lt;li&gt;TSCCA (Dhillon et al., 2012)&lt;/li&gt;
&lt;li&gt;Sparse Random Projections (Li et al., 2006)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于C&amp;amp;W的词向，因为只有基于2007年的维基百科的。所以本文选取了2008-03-01日的维基百科来训练其余5中词向量。这里，所有词向量的维度为50，总共的词典大小为103647&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Evaluation&quot;&gt;2. Evaluation&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Evaluation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;评价词向量主要有两种方式，一种是内部评价（intrinsic evaluation），另一种是外部评价（extrinsic evaluation）。&lt;/p&gt;
&lt;p&gt;内部评价指的是用词的词性，相关性等内部固有关系来评价生成的词向量的好坏。
外部评价指的是用生成的词向量去作为下游任务的输入，看哪种词向量可以更好的实现下游任务。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#20869;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;intrinsic-evaluation&amp;#65289;&quot;&gt;&amp;#20869;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;intrinsic evaluation&amp;#65289;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20869;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;intrinsic-evaluation&amp;#65289;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;对于内部评价，本文采用的绝对的内部评价（absolute intrinsic evaluation）和相对的内部评价（comparative intrinsic evaluation），绝对内部评价有以下方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relatedness：比较生成的词向量的词于词之间的余弦相似度和人类评价的相似度的关系&lt;/li&gt;
&lt;li&gt;Analogy：对于一个y，去找到一个x，使得x:y的关系要和a:b的关系一样&lt;/li&gt;
&lt;li&gt;Categorization：把生成的词向量做聚类，看聚类是否准确&lt;/li&gt;
&lt;li&gt;Selectional preference：确定一个词是某个动词的主语还是宾语&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;评价结果如下,可以看出，绝大多数任务中，CBOW表现最好。但是个别任务里，其他词向量更好&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding1.PNG&quot; alt=&quot;Embedding1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在相对内部评价中，用户直接来判断词向量的好坏。作者的具体做法如下，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选取了词频，词性和词义不同的100个单词（选择10种类别的词，每种类别里有一个形容词，一个动词，4个名词，4个动词）&lt;/li&gt;
&lt;li&gt;找出每个词的n nearest neighbors, 选取rank为1，5，50的neighbor。所以对于6中词向量，对于每一个词，我们分为计算出rank为1，5，50的neighbor。&lt;/li&gt;
&lt;li&gt;让人类来分别评价6中词向量中，rank1，5，50的neighbor里哪个于选定词最近。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结果如下,同样可以看出，没有一种词向量是在所有任务中都表现最好的&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding2.PNG&quot; alt=&quot;Embedding2&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在相似度（relatedness）的比较中，我们对于任意一个单词，我们只找了一个相近的单词，这并不理想（因为每个单词都有很多近义词）。所以作者提出了一种新的衡量方式：Coherence。对于每一个单词，事先选出两个近义词和一个不相关的词，看用生成的词向量能否辨别出不想关的词。&lt;/p&gt;
&lt;p&gt;结果如下,可以看出不同词向量的生成方法，对于不同词频的单词，所得到的结果是不同的&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding3.PNG&quot; alt=&quot;Embedding3&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#22806;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;Extrinsic-evaluation&amp;#65289;&quot;&gt;&amp;#22806;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;Extrinsic evaluation&amp;#65289;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#22806;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;Extrinsic-evaluation&amp;#65289;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;外部评价主要用来测量词向量对于下游任务的贡献。本文选取了以下两种下游任务来评判&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Noun phrase chunking：名词分块&lt;/li&gt;
&lt;li&gt;Sentiment classification：情感分类&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结果如下，对于下游任务，同样的，没有一种词向量可以在所有下游任务中都表现最好，所以对于不同下游任务，我们应该尝试不同词向量的表示&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding4.PNG&quot; alt=&quot;Embedding4&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Frequency-information&quot;&gt;3. Frequency information&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Frequency-information&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;最后，作者通过以下两种实验发现了词向量里面包含词频信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用词向量来预测单词在语料中词频&lt;/li&gt;
&lt;li&gt;对于所有在WordSim-353数据集的单词，研究其K=1000 nearest neighbors和他们在语料中词频的大小排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结果如下,可以看出，我们可以通过词向量来较好的预测单词的词频，其中GloVe和CCA中包含了较多的词频信息。另外单词的词频于其在语料库里的词频排名也有很强的相关性&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding5.PNG&quot; alt=&quot;Embedding5&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-&amp;#24605;&amp;#32771;&quot;&gt;4. &amp;#24605;&amp;#32771;&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-&amp;#24605;&amp;#32771;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;通过本文，我们发现没有任何一种词向量可以在所有任务中都表现的最好，所以每个单词应该不存在一种绝对正确的词向量。那么，词向量是否是用来表示单词的最好方式呢，我对此表示疑问。以后很有可能会发现一种新的表示单词的方式。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">文本预处理方法汇总</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/03/Text-Preprocessing.html" rel="alternate" type="text/html" title="文本预处理方法汇总" /><published>2020-07-03T00:00:00-05:00</published><updated>2020-07-03T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/07/03/Text-Preprocessing</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/03/Text-Preprocessing.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-03-Text-Preprocessing.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文汇总各种文本预处理的方法，皆在方便自己快速查找。
original link is here &lt;a href=&quot;https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&quot;&gt;https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;all-to-upper-case-or-lowwer-case&quot;&gt;all to upper case or lowwer case&lt;a class=&quot;anchor-link&quot; href=&quot;#all-to-upper-case-or-lowwer-case&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;AbcdEfG&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;&amp;#39;abcdefg&amp;#39;&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;replace-numbers-or-remove-numbers&quot;&gt;replace numbers or remove numbers&lt;a class=&quot;anchor-link&quot; href=&quot;#replace-numbers-or-remove-numbers&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;\d+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Box A contains  red and  white balls, while Box B contains  red and  blue balls.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Remove-Punctuation&quot;&gt;Remove Punctuation&lt;a class=&quot;anchor-link&quot; href=&quot;#Remove-Punctuation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;This &amp;amp;is [an] example? &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{of}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; string. with.? punctuation!!!!&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;punctuation_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;punctuation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;translate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;punctuation_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;This is an example of string with punctuation
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Remove-Whitespace&quot;&gt;Remove Whitespace&lt;a class=&quot;anchor-link&quot; href=&quot;#Remove-Whitespace&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;   This has a lot whitespace    &amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;This has a lot whitespace
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Tokenization&quot;&gt;Tokenization&lt;a class=&quot;anchor-link&quot; href=&quot;#Tokenization&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/3220/1*ffMYw8aujrmyxfA55Zm3Jg.jpeg&quot; alt=&quot;tokenization&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.tokenize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WhitespaceTokenizer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WhitespaceTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;I love you&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;[&amp;#39;I&amp;#39;, &amp;#39;love&amp;#39;, &amp;#39;you&amp;#39;]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Remove-Stop-words&quot;&gt;Remove Stop words&lt;a class=&quot;anchor-link&quot; href=&quot;#Remove-Stop-words&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.corpus&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;NLTK is a leading platform for building Python programs to work with human language data.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;[&amp;#39;NLTK&amp;#39;, &amp;#39;leading&amp;#39;, &amp;#39;platform&amp;#39;, &amp;#39;building&amp;#39;, &amp;#39;Python&amp;#39;, &amp;#39;programs&amp;#39;, &amp;#39;work&amp;#39;, &amp;#39;human&amp;#39;, &amp;#39;language&amp;#39;, &amp;#39;data.&amp;#39;]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.stop_words&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ENGLISH_STOP_WORDS&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ENGLISH_STOP_WORDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;frozenset({&amp;#39;together&amp;#39;, &amp;#39;seemed&amp;#39;, &amp;#39;she&amp;#39;, &amp;#39;hers&amp;#39;, &amp;#39;ie&amp;#39;, &amp;#39;may&amp;#39;, &amp;#39;becoming&amp;#39;, &amp;#39;though&amp;#39;, &amp;#39;everything&amp;#39;, &amp;#39;only&amp;#39;, &amp;#39;somewhere&amp;#39;, &amp;#39;at&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;very&amp;#39;, &amp;#39;few&amp;#39;, &amp;#39;many&amp;#39;, &amp;#39;whither&amp;#39;, &amp;#39;my&amp;#39;, &amp;#39;onto&amp;#39;, &amp;#39;now&amp;#39;, &amp;#39;keep&amp;#39;, &amp;#39;mill&amp;#39;, &amp;#39;this&amp;#39;, &amp;#39;than&amp;#39;, &amp;#39;once&amp;#39;, &amp;#39;seems&amp;#39;, &amp;#39;might&amp;#39;, &amp;#39;please&amp;#39;, &amp;#39;these&amp;#39;, &amp;#39;among&amp;#39;, &amp;#39;hence&amp;#39;, &amp;#39;thus&amp;#39;, &amp;#39;something&amp;#39;, &amp;#39;rather&amp;#39;, &amp;#39;how&amp;#39;, &amp;#39;whereas&amp;#39;, &amp;#39;whence&amp;#39;, &amp;#39;everywhere&amp;#39;, &amp;#39;last&amp;#39;, &amp;#39;anyone&amp;#39;, &amp;#39;never&amp;#39;, &amp;#39;somehow&amp;#39;, &amp;#39;another&amp;#39;, &amp;#39;herself&amp;#39;, &amp;#39;i&amp;#39;, &amp;#39;detail&amp;#39;, &amp;#39;two&amp;#39;, &amp;#39;elsewhere&amp;#39;, &amp;#39;give&amp;#39;, &amp;#39;nowhere&amp;#39;, &amp;#39;myself&amp;#39;, &amp;#39;me&amp;#39;, &amp;#39;some&amp;#39;, &amp;#39;of&amp;#39;, &amp;#39;everyone&amp;#39;, &amp;#39;first&amp;#39;, &amp;#39;yourselves&amp;#39;, &amp;#39;himself&amp;#39;, &amp;#39;meanwhile&amp;#39;, &amp;#39;serious&amp;#39;, &amp;#39;found&amp;#39;, &amp;#39;hereafter&amp;#39;, &amp;#39;much&amp;#39;, &amp;#39;becomes&amp;#39;, &amp;#39;nobody&amp;#39;, &amp;#39;thin&amp;#39;, &amp;#39;namely&amp;#39;, &amp;#39;find&amp;#39;, &amp;#39;indeed&amp;#39;, &amp;#39;thru&amp;#39;, &amp;#39;those&amp;#39;, &amp;#39;no&amp;#39;, &amp;#39;noone&amp;#39;, &amp;#39;both&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;hasnt&amp;#39;, &amp;#39;own&amp;#39;, &amp;#39;not&amp;#39;, &amp;#39;amoungst&amp;#39;, &amp;#39;empty&amp;#39;, &amp;#39;then&amp;#39;, &amp;#39;their&amp;#39;, &amp;#39;again&amp;#39;, &amp;#39;further&amp;#39;, &amp;#39;itself&amp;#39;, &amp;#39;most&amp;#39;, &amp;#39;hereby&amp;#39;, &amp;#39;up&amp;#39;, &amp;#39;wherein&amp;#39;, &amp;#39;to&amp;#39;, &amp;#39;thereupon&amp;#39;, &amp;#39;across&amp;#39;, &amp;#39;on&amp;#39;, &amp;#39;along&amp;#39;, &amp;#39;except&amp;#39;, &amp;#39;done&amp;#39;, &amp;#39;anyway&amp;#39;, &amp;#39;had&amp;#39;, &amp;#39;go&amp;#39;, &amp;#39;any&amp;#39;, &amp;#39;will&amp;#39;, &amp;#39;often&amp;#39;, &amp;#39;upon&amp;#39;, &amp;#39;three&amp;#39;, &amp;#39;fire&amp;#39;, &amp;#39;neither&amp;#39;, &amp;#39;anyhow&amp;#39;, &amp;#39;either&amp;#39;, &amp;#39;there&amp;#39;, &amp;#39;forty&amp;#39;, &amp;#39;re&amp;#39;, &amp;#39;per&amp;#39;, &amp;#39;formerly&amp;#39;, &amp;#39;beside&amp;#39;, &amp;#39;seeming&amp;#39;, &amp;#39;inc&amp;#39;, &amp;#39;amount&amp;#39;, &amp;#39;un&amp;#39;, &amp;#39;could&amp;#39;, &amp;#39;out&amp;#39;, &amp;#39;against&amp;#39;, &amp;#39;twelve&amp;#39;, &amp;#39;system&amp;#39;, &amp;#39;mostly&amp;#39;, &amp;#39;down&amp;#39;, &amp;#39;other&amp;#39;, &amp;#39;between&amp;#39;, &amp;#39;thereafter&amp;#39;, &amp;#39;below&amp;#39;, &amp;#39;full&amp;#39;, &amp;#39;our&amp;#39;, &amp;#39;would&amp;#39;, &amp;#39;anything&amp;#39;, &amp;#39;are&amp;#39;, &amp;#39;almost&amp;#39;, &amp;#39;but&amp;#39;, &amp;#39;bottom&amp;#39;, &amp;#39;your&amp;#39;, &amp;#39;made&amp;#39;, &amp;#39;see&amp;#39;, &amp;#39;until&amp;#39;, &amp;#39;eg&amp;#39;, &amp;#39;beforehand&amp;#39;, &amp;#39;as&amp;#39;, &amp;#39;therefore&amp;#39;, &amp;#39;cannot&amp;#39;, &amp;#39;enough&amp;#39;, &amp;#39;what&amp;#39;, &amp;#39;became&amp;#39;, &amp;#39;con&amp;#39;, &amp;#39;through&amp;#39;, &amp;#39;front&amp;#39;, &amp;#39;six&amp;#39;, &amp;#39;from&amp;#39;, &amp;#39;all&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;put&amp;#39;, &amp;#39;someone&amp;#39;, &amp;#39;throughout&amp;#39;, &amp;#39;former&amp;#39;, &amp;#39;has&amp;#39;, &amp;#39;still&amp;#39;, &amp;#39;due&amp;#39;, &amp;#39;next&amp;#39;, &amp;#39;fifteen&amp;#39;, &amp;#39;off&amp;#39;, &amp;#39;and&amp;#39;, &amp;#39;cant&amp;#39;, &amp;#39;alone&amp;#39;, &amp;#39;amongst&amp;#39;, &amp;#39;besides&amp;#39;, &amp;#39;side&amp;#39;, &amp;#39;about&amp;#39;, &amp;#39;we&amp;#39;, &amp;#39;he&amp;#39;, &amp;#39;eleven&amp;#39;, &amp;#39;always&amp;#39;, &amp;#39;was&amp;#39;, &amp;#39;whatever&amp;#39;, &amp;#39;none&amp;#39;, &amp;#39;whenever&amp;#39;, &amp;#39;whole&amp;#39;, &amp;#39;where&amp;#39;, &amp;#39;her&amp;#39;, &amp;#39;above&amp;#39;, &amp;#39;also&amp;#39;, &amp;#39;ourselves&amp;#39;, &amp;#39;four&amp;#39;, &amp;#39;top&amp;#39;, &amp;#39;fill&amp;#39;, &amp;#39;although&amp;#39;, &amp;#39;which&amp;#39;, &amp;#39;move&amp;#39;, &amp;#39;sixty&amp;#39;, &amp;#39;thence&amp;#39;, &amp;#39;were&amp;#39;, &amp;#39;nothing&amp;#39;, &amp;#39;bill&amp;#39;, &amp;#39;however&amp;#39;, &amp;#39;you&amp;#39;, &amp;#39;hundred&amp;#39;, &amp;#39;same&amp;#39;, &amp;#39;must&amp;#39;, &amp;#39;ltd&amp;#39;, &amp;#39;been&amp;#39;, &amp;#39;they&amp;#39;, &amp;#39;whereupon&amp;#39;, &amp;#39;sincere&amp;#39;, &amp;#39;name&amp;#39;, &amp;#39;because&amp;#39;, &amp;#39;hereupon&amp;#39;, &amp;#39;others&amp;#39;, &amp;#39;who&amp;#39;, &amp;#39;cry&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;while&amp;#39;, &amp;#39;if&amp;#39;, &amp;#39;too&amp;#39;, &amp;#39;since&amp;#39;, &amp;#39;or&amp;#39;, &amp;#39;sometimes&amp;#39;, &amp;#39;therein&amp;#39;, &amp;#39;without&amp;#39;, &amp;#39;ten&amp;#39;, &amp;#39;eight&amp;#39;, &amp;#39;via&amp;#39;, &amp;#39;five&amp;#39;, &amp;#39;into&amp;#39;, &amp;#39;ours&amp;#39;, &amp;#39;co&amp;#39;, &amp;#39;yours&amp;#39;, &amp;#39;an&amp;#39;, &amp;#39;themselves&amp;#39;, &amp;#39;us&amp;#39;, &amp;#39;for&amp;#39;, &amp;#39;thick&amp;#39;, &amp;#39;thereby&amp;#39;, &amp;#39;so&amp;#39;, &amp;#39;more&amp;#39;, &amp;#39;under&amp;#39;, &amp;#39;am&amp;#39;, &amp;#39;by&amp;#39;, &amp;#39;less&amp;#39;, &amp;#39;ever&amp;#39;, &amp;#39;otherwise&amp;#39;, &amp;#39;whoever&amp;#39;, &amp;#39;nine&amp;#39;, &amp;#39;even&amp;#39;, &amp;#39;wherever&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;yourself&amp;#39;, &amp;#39;herein&amp;#39;, &amp;#39;every&amp;#39;, &amp;#39;part&amp;#39;, &amp;#39;each&amp;#39;, &amp;#39;already&amp;#39;, &amp;#39;such&amp;#39;, &amp;#39;in&amp;#39;, &amp;#39;afterwards&amp;#39;, &amp;#39;be&amp;#39;, &amp;#39;least&amp;#39;, &amp;#39;why&amp;#39;, &amp;#39;anywhere&amp;#39;, &amp;#39;with&amp;#39;, &amp;#39;them&amp;#39;, &amp;#39;perhaps&amp;#39;, &amp;#39;latter&amp;#39;, &amp;#39;seem&amp;#39;, &amp;#39;back&amp;#39;, &amp;#39;during&amp;#39;, &amp;#39;can&amp;#39;, &amp;#39;else&amp;#39;, &amp;#39;being&amp;#39;, &amp;#39;over&amp;#39;, &amp;#39;whose&amp;#39;, &amp;#39;within&amp;#39;, &amp;#39;moreover&amp;#39;, &amp;#39;whereby&amp;#39;, &amp;#39;fifty&amp;#39;, &amp;#39;mine&amp;#39;, &amp;#39;several&amp;#39;, &amp;#39;get&amp;#39;, &amp;#39;its&amp;#39;, &amp;#39;well&amp;#39;, &amp;#39;take&amp;#39;, &amp;#39;whom&amp;#39;, &amp;#39;after&amp;#39;, &amp;#39;it&amp;#39;, &amp;#39;third&amp;#39;, &amp;#39;describe&amp;#39;, &amp;#39;whereafter&amp;#39;, &amp;#39;nor&amp;#39;, &amp;#39;that&amp;#39;, &amp;#39;before&amp;#39;, &amp;#39;interest&amp;#39;, &amp;#39;when&amp;#39;, &amp;#39;call&amp;#39;, &amp;#39;sometime&amp;#39;, &amp;#39;nevertheless&amp;#39;, &amp;#39;toward&amp;#39;, &amp;#39;show&amp;#39;, &amp;#39;latterly&amp;#39;, &amp;#39;twenty&amp;#39;, &amp;#39;yet&amp;#39;, &amp;#39;couldnt&amp;#39;, &amp;#39;have&amp;#39;, &amp;#39;him&amp;#39;, &amp;#39;around&amp;#39;, &amp;#39;become&amp;#39;, &amp;#39;do&amp;#39;, &amp;#39;behind&amp;#39;, &amp;#39;beyond&amp;#39;, &amp;#39;towards&amp;#39;, &amp;#39;here&amp;#39;, &amp;#39;de&amp;#39;, &amp;#39;should&amp;#39;, &amp;#39;whether&amp;#39;, &amp;#39;etc&amp;#39;})
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Stemming-&amp;amp;-Remove-sparse-terms-and-particular-words&quot;&gt;Stemming &amp;amp; Remove sparse terms and particular words&lt;a class=&quot;anchor-link&quot; href=&quot;#Stemming-&amp;amp;-Remove-sparse-terms-and-particular-words&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/3492/1*JpOXoNSFkZ0sjqPYT2U4cA.jpeg&quot; alt=&quot;Stemming&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.stem&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PorterStemmer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.tokenize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PorterStemmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;There are several types of stemming algorithms.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;there
are
sever
type
of
stem
algorithm
.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Lemmatization&quot;&gt;Lemmatization&lt;a class=&quot;anchor-link&quot; href=&quot;#Lemmatization&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.stem&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WordNetLemmatizer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.tokenize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lemmatizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WordNetLemmatizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;been had done languages cities mice&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemmatizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemmatize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;been
had
done
language
city
mouse
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-of-speech-tagging-(POS)&quot;&gt;Part of speech tagging (POS)&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-of-speech-tagging-(POS)&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_tag&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Parts of speech examples: an article, to write, interesting, easily, and, of&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;[(&amp;#39;Parts&amp;#39;, &amp;#39;NNS&amp;#39;), (&amp;#39;of&amp;#39;, &amp;#39;IN&amp;#39;), (&amp;#39;speech&amp;#39;, &amp;#39;NN&amp;#39;), (&amp;#39;examples&amp;#39;, &amp;#39;NNS&amp;#39;), (&amp;#39;:&amp;#39;, &amp;#39;:&amp;#39;), (&amp;#39;an&amp;#39;, &amp;#39;DT&amp;#39;), (&amp;#39;article&amp;#39;, &amp;#39;NN&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;to&amp;#39;, &amp;#39;TO&amp;#39;), (&amp;#39;write&amp;#39;, &amp;#39;VB&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;interesting&amp;#39;, &amp;#39;VBG&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;easily&amp;#39;, &amp;#39;RB&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;and&amp;#39;, &amp;#39;CC&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;of&amp;#39;, &amp;#39;IN&amp;#39;)]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Chunking-(shallow-parsing)&quot;&gt;Chunking (shallow parsing)&lt;a class=&quot;anchor-link&quot; href=&quot;#Chunking-(shallow-parsing)&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) [23]. Chunking tools: NLTK, TreeTagger chunker, Apache OpenNLP, General Architecture for Text Engineering (GATE), FreeLing.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">From Word Embeddings To Document Distances详解</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances.html" rel="alternate" type="text/html" title="From Word Embeddings To Document Distances详解" /><published>2020-06-11T00:00:00-05:00</published><updated>2020-06-11T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-11-From-Word-Embeddings-To-Document-Distances.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-&amp;#31616;&amp;#20171;&quot;&gt;1. &amp;#31616;&amp;#20171;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-&amp;#31616;&amp;#20171;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;《From Word Embeddings To Document Distances》这篇文章，发表于2014年。作者在word2vec的基础上提出了一种衡量文章相似度的尺度，Word Mover's Distance(WDM)。 WDM用来描述两篇文章的词向量之间的距离。这里的“距离”作者用了“旅行距离”来描述，意思是从一篇文章的词向量转换到另一篇文章的词向量的最短距离。在此距离的基础上，作者提出了衡量文章相似度的算法。从实验可以看出，该方法优于当时的SOTA。&lt;/p&gt;
&lt;p&gt;下文会按照作者的思路来详细介绍WDM的定义和文章相似度的计算方法（站在作者角度来想）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在计算文章相似度的问题中，前人的方法只考虑词频，没有考虑文章的意义，把word2vec用到文章相似度的评价里会不会有所提高呢？&lt;/li&gt;
&lt;li&gt;那么我来定义了一种新的计算文章相似度的方法WMD。&lt;/li&gt;
&lt;li&gt;可是WMD的计算复杂度太高。那我提出计算下限的方法WCD和RWMD。&lt;/li&gt;
&lt;li&gt;有了WCD和RWMD，然后通过Prefetch和Prune来找到一个文章的k nearest neighbors。&lt;/li&gt;
&lt;li&gt;看看我的实验结果多牛叉。超过了当时的SOTA。&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-&amp;#35770;&amp;#25991;&amp;#35814;&amp;#35299;&quot;&gt;2. &amp;#35770;&amp;#25991;&amp;#35814;&amp;#35299;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-&amp;#35770;&amp;#25991;&amp;#35814;&amp;#35299;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;&amp;#19968;&amp;#12289;Word2Vec-Embedding&quot;&gt;&amp;#19968;&amp;#12289;Word2Vec Embedding&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#19968;&amp;#12289;Word2Vec-Embedding&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在2013年，Mikolov et al提出了word2vec，这个在当时引起了很大反响。该研究用向量来表示每个单词，从数学上更好的表示了单词的意义。传统上表示文章，大多数都是用BOW或者TFIDF的手法，这些手法对于词的意义有欠考虑，所以在衡量文章相似度上，往往停留在单词一致性的表层上，文章意思的相似性得不到很大的表现。作者关注到word2vec的发展，然后将该方法应用到了文章相似度的表达上。这里我们首先介绍以下word2vec。&lt;/p&gt;
&lt;p&gt;简单来说，word2vec是一种通过学习神经网络来寻找词向量表示的一种方法。具体来说word2vec的skip-gram模型是通过构建一个单层神经网络（input layer, projection layer, output layer)来预测每个单词的相邻单词。通过学习该神经网络，得到的projection layer即为每个单词的词向量。训练目标是使每个单词的相邻单词的出现概率最大化。该概率可以用下式表示
$$
\frac{1}{T}\sum_{t=1}^T\sum_{j \in nb(t)}logp(w_j|w_t) \tag{1}
$$&lt;/p&gt;
&lt;p&gt;这里T为单词数，nb(t)表示单词$w_t$的相邻单词，$p(w_j|w_t)$用hierarchical softmax来提高训练速度。&lt;/p&gt;
&lt;h3 id=&quot;&amp;#20108;&amp;#12289;Word-Move's-Distance&quot;&gt;&amp;#20108;&amp;#12289;Word Move's Distance&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20108;&amp;#12289;Word-Move's-Distance&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先我们用词向量来定义Word travel cost。假设$x_i, x_j$表示word $i$和word $j$的词向量。我们用$c(i, j) = ||x_i - x_j||_2$来表示从一个词到另一个词的&quot;旅行距离&quot;（Word travel cost）。有了词于词之间的距离，接下来我们来定义文章间距离。设$d$, $d'$为两篇文章的nBow (normalized bag of words)表示。令$T_{ij}, T \in R^{n\times n}$来表示文章d的词i到文章d'的距离。我们令词i到文章d'的所有词的距离之和为$\sum_jT_{ij} = d_i$, 反之文章d'的词j到文章d的所有词之和为$\sum_iT_{ij}=d'_j$。最终，我们定义两个文章的距离为从一篇文章d到另一篇文章d‘距离的加权累加的最小值。用数学描述为
$$
\min_{T&amp;gt;=0}\sum_{i,j=1}^nT_{ij}c(i,j)　\\
subject \ to: \sum_{j=1}^nT_{i,j} = d_i, \forall_i \in \{1,...,n\} \\
\sum_{i=1}^nT_{i,j} = d'_j, \forall_j \in \{1,...,n\} \tag{2}
$$
所以，求解文章与文章之间的WMD转化为了一个最优化问题。&lt;/p&gt;
&lt;p&gt;通过下图简要举例介绍词数相同时文章间距离和词数不同时文章距离的情况。在上半部分，去除了（the, to ,in, a）等等stopwords之后，D0,D1,D2都是由4个不同的词构成的。所以，所有的词的$d_i=0.25$，这里的箭头表示的是$T_{ij}c(i,j)$, 由于词向量空间上，比起band，Obama离President更近，这里的分数也很好的反应了该结果。在图的下半部分，我们可以看出，当词数不同时，一个词可能会映射到多个相似的词。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/WMD1.png&quot; alt=&quot;WMD1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#19977;&amp;#12289;&amp;#31867;&amp;#27604;&amp;#36816;&amp;#36755;&amp;#38382;&amp;#39064;&quot;&gt;&amp;#19977;&amp;#12289;&amp;#31867;&amp;#27604;&amp;#36816;&amp;#36755;&amp;#38382;&amp;#39064;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#19977;&amp;#12289;&amp;#31867;&amp;#27604;&amp;#36816;&amp;#36755;&amp;#38382;&amp;#39064;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;运输问题的典型情况是研究单一品种物质的运输调度问题：设某种物品有m个产地$A_1，A_2，···，A_m$，各产地的产量分别是$a_1，a_2，···，a_m$,有n个销地$B_1，B_2，···，B_n$，各个销地的销量分别为$b_1，b_2，···，b_n$。假定从产地$A_i(i=1,2,···,m)$向销地$B_j(j=1,2,···,n)$运输单位物品的运价为$c_{ij}$，怎么调运这些物品才能使总运费最小？可以看出，文章的WMD计算和运输问题是完全对应的。这里产地$A_1，A_2，···，A_m$对应于文章A的单词，$a_1，a_2，···，a_m$为单词的词频。同理$B_1，B_2，···，B_n$为文章B的单词，$b_1，b_2，···，b_n$为对应的词频。运价$c_{ij}$对应于文章A的单词和文章B的单词间的欧拉距离。运输问题可以描述成以下线性规划问题。&lt;/p&gt;
$$
\min_z = \sum_{i=1}^m\sum_{j=1}^nc_{ij}x_{ij} \\
subject \ to: \sum_{j=1}^nx_{i,j} = a_i, \forall_i \in \{1,...,n\} \\
\sum_{i=1}^nx_{i,j} = b_j, \forall_j \in \{1,...,n\}  \\
x_{ij} \geq 0 \tag{3}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#22235;&amp;#12289;Fast-Distance-Computation&quot;&gt;&amp;#22235;&amp;#12289;Fast Distance Computation&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#22235;&amp;#12289;Fast-Distance-Computation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;解决上述WMD最优化问题的复杂度时$O(p^3\log p)$, p为去重后单词数。可以想象，当文章有很多单词时，计算WMD会变的非常困难。为此，作者提出了两种求解WMD下限的方法&lt;/p&gt;
&lt;h5 id=&quot;WCD&amp;#65306;&amp;#36890;&amp;#36807;&amp;#19977;&amp;#35282;&amp;#19981;&amp;#31561;&amp;#24335;$||x+y||-\leq-||x||-+-||y||$&amp;#65292;&amp;#25105;&amp;#20204;&amp;#21487;&amp;#20197;&amp;#31616;&amp;#21333;&amp;#30340;&amp;#24471;&amp;#21040;&quot;&gt;WCD&amp;#65306;&amp;#36890;&amp;#36807;&amp;#19977;&amp;#35282;&amp;#19981;&amp;#31561;&amp;#24335;$||x+y|| \leq ||x|| + ||y||$&amp;#65292;&amp;#25105;&amp;#20204;&amp;#21487;&amp;#20197;&amp;#31616;&amp;#21333;&amp;#30340;&amp;#24471;&amp;#21040;&lt;a class=&quot;anchor-link&quot; href=&quot;#WCD&amp;#65306;&amp;#36890;&amp;#36807;&amp;#19977;&amp;#35282;&amp;#19981;&amp;#31561;&amp;#24335;$||x+y||-\leq-||x||-+-||y||$&amp;#65292;&amp;#25105;&amp;#20204;&amp;#21487;&amp;#20197;&amp;#31616;&amp;#21333;&amp;#30340;&amp;#24471;&amp;#21040;&quot;&gt; &lt;/a&gt;&lt;/h5&gt;$$
\sum_{i,j=1}^nT_{ij}c(i,j) = \sum_{i,j=1}^nT_{ij}||x_i-x'_j||_2 \\
= \sum_{i,j=1}||T_{ij}(x_i-x'_j)||_2 \geq ||\sum_{i,j=1}^nT_{ij}(x_i-x'_j)||_2 \\
= ||\sum_{i=1}^n(\sum_{j=1}^nT_{ij})x_i-\sum_{j=1}^n(\sum_{j=1}^nT_{ij})x'_j||_2 \\
= ||\sum_{i=1}^nd_ix_i - \sum_{j=1}^nd'_jx'_j||_2 \tag{4}
$$&lt;p&gt;作者将这个距离称为Word Centroid Distance（WCD），可以看出计算这个WMD的下限WCD非常快，算法复杂度是O(dp), d为词向量的维度，p为去重后的单词数。在寻找某篇文章的k个最相似的文章时，WCD可以用于寻找有利的候补，从而提高算法效率。&lt;/p&gt;
&lt;h5 id=&quot;RWMD&amp;#65306;&amp;#34429;&amp;#28982;WCD&amp;#24456;&amp;#23481;&amp;#26131;&amp;#35745;&amp;#31639;&amp;#65292;&amp;#20294;&amp;#26159;&amp;#22240;&amp;#20026;&amp;#20182;&amp;#32473;&amp;#20986;&amp;#30340;&amp;#19979;&amp;#38480;&amp;#22826;&amp;#20302;&amp;#65292;&amp;#20316;&amp;#32773;&amp;#22312;&amp;#36825;&amp;#37324;&amp;#25552;&amp;#20986;&amp;#20102;&amp;#19968;&amp;#31181;&amp;#26356;&amp;#21152;&amp;#36924;&amp;#36817;&amp;#30495;&amp;#23454;&amp;#20540;&amp;#30340;&amp;#19979;&amp;#38480;Relaxed-word-moving-distance&amp;#12290;&quot;&gt;RWMD&amp;#65306;&amp;#34429;&amp;#28982;WCD&amp;#24456;&amp;#23481;&amp;#26131;&amp;#35745;&amp;#31639;&amp;#65292;&amp;#20294;&amp;#26159;&amp;#22240;&amp;#20026;&amp;#20182;&amp;#32473;&amp;#20986;&amp;#30340;&amp;#19979;&amp;#38480;&amp;#22826;&amp;#20302;&amp;#65292;&amp;#20316;&amp;#32773;&amp;#22312;&amp;#36825;&amp;#37324;&amp;#25552;&amp;#20986;&amp;#20102;&amp;#19968;&amp;#31181;&amp;#26356;&amp;#21152;&amp;#36924;&amp;#36817;&amp;#30495;&amp;#23454;&amp;#20540;&amp;#30340;&amp;#19979;&amp;#38480;Relaxed word moving distance&amp;#12290;&lt;a class=&quot;anchor-link&quot; href=&quot;#RWMD&amp;#65306;&amp;#34429;&amp;#28982;WCD&amp;#24456;&amp;#23481;&amp;#26131;&amp;#35745;&amp;#31639;&amp;#65292;&amp;#20294;&amp;#26159;&amp;#22240;&amp;#20026;&amp;#20182;&amp;#32473;&amp;#20986;&amp;#30340;&amp;#19979;&amp;#38480;&amp;#22826;&amp;#20302;&amp;#65292;&amp;#20316;&amp;#32773;&amp;#22312;&amp;#36825;&amp;#37324;&amp;#25552;&amp;#20986;&amp;#20102;&amp;#19968;&amp;#31181;&amp;#26356;&amp;#21152;&amp;#36924;&amp;#36817;&amp;#30495;&amp;#23454;&amp;#20540;&amp;#30340;&amp;#19979;&amp;#38480;Relaxed-word-moving-distance&amp;#12290;&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;想法很简单，就是去掉一个WMD的约束条件。假设去除第二个约束条件，原式变成
$$
\min_{T&amp;gt;=0}\sum_{i,j=1}^nT_{ij}c(i,j)　\\
subject \ to: \sum_{j=1}^nT_{i,j} = d_i, \forall_i \in \{1,...,n\} \tag{5}
$$
因为在所有满足WMD的解之中，肯定也可以找到一个满足RWMD的解，所以RWMD可以作为WMD的一个下限。从直观上理解，去掉约束条件后，文章B的词语并不一定都要被映射到，我们只要让每一个文章A的词语都旅行到文章B即可（举个极端的例子，文章A的所有词语都映射到文章B的某一个词）。类比于运输问题，即是我们只对产量有要求，对销量没有要求。在这种情况下，可想而知，我们只要让文章A的所有词都映射到离其词向量最近的点即可得到最优解。
$$
T_{ij}^* = \begin{cases}
d_{i}\ if j=argmin_{j}c(i,j) \\
0 \ otherwise.
\end{cases} \tag{6}
$$
可以得到
$$
\sum_jT_{ij}c(i,j) \geq \sum_jT_{ij}^*c(i,j) \tag{7}
$$
因为计算RWMD只需要找个离每个词语的最近的词，他的复杂度是$O(p^2)$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#20116;&amp;#12289;Prefetch-and-prune&quot;&gt;&amp;#20116;&amp;#12289;Prefetch and prune&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20116;&amp;#12289;Prefetch-and-prune&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;作者利用上述WCD和RWMD提出了一种快速查找某文章的k nearest neighbors的算法。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算所有文章对该文章的WCD，然后按升序排序&lt;/li&gt;
&lt;li&gt;计算前k个文章的WMD（计算k nearest neighbors的WMD）&lt;/li&gt;
&lt;li&gt;然后来计算剩余文章的RWMD，如果某文章的RWMD大于第k个文章的WMD，那么就不再考虑该文章。反之，我们更新k nearest neighbors&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;&amp;#20845;&amp;#12289;&amp;#32467;&amp;#26524;&quot;&gt;&amp;#20845;&amp;#12289;&amp;#32467;&amp;#26524;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20845;&amp;#12289;&amp;#32467;&amp;#26524;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过以下结果可知，这种计算文章距离的算法击败了当时大部分的SOAT。
&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/WMD2.png&quot; alt=&quot;WDM2&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-&amp;#24605;&amp;#32771;&quot;&gt;3. &amp;#24605;&amp;#32771;&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-&amp;#24605;&amp;#32771;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在有了word2vec后，计算文章距离的想法其实很容易就能够想到。不过如果没有近似算法，那么就没有实际利用价值。做科研，理论和实践两手都要硬啊。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &lt;a href=&quot;http://proceedings.mlr.press/v37/kusnerb15.pdf&quot;&gt;From Word Embeddings To Document Distances&lt;/a&gt;&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &lt;a href=&quot;https://baike.baidu.com/item/%E8%BF%90%E8%BE%93%E9%97%AE%E9%A2%98/12734790?fr=aladdin&quot;&gt;百度百科：运输问题&lt;/a&gt;&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry></feed>