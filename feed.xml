<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://xubujie.github.io/DataScienceBlog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xubujie.github.io/DataScienceBlog/" rel="alternate" type="text/html" /><updated>2020-08-01T08:56:43-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/feed.xml</id><title type="html">Jay’s Blog</title><subtitle>Sharing my learning and idea</subtitle><entry><title type="html">Sequenc to Sequence Learning with Nueral Networks</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks.html" rel="alternate" type="text/html" title="Sequenc to Sequence Learning with Nueral Networks" /><published>2020-07-31T00:00:00-05:00</published><updated>2020-07-31T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-31-Sequence-to-Sequence-Learning-with-Nueral-Networks.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;这篇文章发表于2014年底。我们所熟知的seq2seq模型由该文章提出。本文提出了一种解决由一个序列预测另一个序列问题的端到端的模型(seq2seq模型)。通过在WMT'14数据集上的测试，证明了端到端的seq2seq模型可以达到SOAT的精度。&lt;/p&gt;
&lt;p&gt;接下来我们主要介绍以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;背景和模型概要&lt;/li&gt;
&lt;li&gt;实验设计和工程实现&lt;/li&gt;
&lt;li&gt;实验结果&lt;/li&gt;
&lt;li&gt;总结&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-&amp;#32972;&amp;#26223;&amp;#21644;&amp;#27169;&amp;#22411;&amp;#27010;&amp;#35201;&quot;&gt;1. &amp;#32972;&amp;#26223;&amp;#21644;&amp;#27169;&amp;#22411;&amp;#27010;&amp;#35201;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-&amp;#32972;&amp;#26223;&amp;#21644;&amp;#27169;&amp;#22411;&amp;#27010;&amp;#35201;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;传统的DNN虽然在很多领域取得了很好的成绩，但是DNN之只能适用于输入和输出的维度都确定的情况下。对于输入和输出长度不固定的问题，传统的DNN束手无策。
在本篇文章，作者为了解决此问题，提出了一种seq2seq的模型来处理。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq.PNG&quot; alt=&quot;seq2seq&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;如上图所示，作者利用一个LSTM来编码输入序列，然后用另一个LSTM来对输出进行解码。这里用&quot;EOS&quot;来表示句尾，从而使得输出的序列长度由模型来决定。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-&amp;#23454;&amp;#39564;&amp;#35774;&amp;#35745;&amp;#21644;&amp;#24037;&amp;#31243;&amp;#23454;&amp;#29616;&quot;&gt;2. &amp;#23454;&amp;#39564;&amp;#35774;&amp;#35745;&amp;#21644;&amp;#24037;&amp;#31243;&amp;#23454;&amp;#29616;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-&amp;#23454;&amp;#39564;&amp;#35774;&amp;#35745;&amp;#21644;&amp;#24037;&amp;#31243;&amp;#23454;&amp;#29616;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.1-&amp;#25968;&amp;#25454;&amp;#21644;&amp;#35780;&amp;#20215;&amp;#25351;&amp;#26631;&quot;&gt;2.1 &amp;#25968;&amp;#25454;&amp;#21644;&amp;#35780;&amp;#20215;&amp;#25351;&amp;#26631;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.1-&amp;#25968;&amp;#25454;&amp;#21644;&amp;#35780;&amp;#20215;&amp;#25351;&amp;#26631;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;实验所采用的数据集来自WMT'14里英文到法文的翻译任务的数据。总共句子数为12M，其中法文单词有348M，英文单词有304M。另外作者事先给定了两种语言的词库，英文词库包含160,000常用词，法文词库包含80,000常用词，其他单词用&quot;UNK&quot;表示。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;BLEU来评价机器翻译的好坏。BLEU公式如下&lt;/p&gt;
$$
BLEU = BP \times e^{\sum_{i=1}^nw_i\log(p_i)}
$$$$
BP = 
\begin{cases}
1 &amp;amp;\text{if c&amp;gt;r} \\
e^{1-r/c} &amp;amp;c \leq r
\end{cases}
$$&lt;p&gt;这里r为一个参考长度，当翻译结果长度为r时，不需要对结果进行惩罚&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.2-&amp;#35757;&amp;#32451;&amp;#35814;&amp;#24773;&quot;&gt;2.2 &amp;#35757;&amp;#32451;&amp;#35814;&amp;#24773;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.2-&amp;#35757;&amp;#32451;&amp;#35814;&amp;#24773;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;模型由4层LSTM作为编码器，另外一个4层LSTM作为解码器。输入的词向量维度为1000。其他实验中的细节如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用一个范围为-0.08到0.08的均一分布来初始化LSTM的参数&lt;/li&gt;
&lt;li&gt;用SGD进行训练，初始5个epoch的学习率设为0.7，之后每半个epoch学习率减半，总共训练了7.5个epoch&lt;/li&gt;
&lt;li&gt;batch size为128， 每个batch里面的句子长度尽量相近&lt;/li&gt;
&lt;li&gt;为了防止梯度爆炸，采取了梯度裁剪&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;此外，作者还提到了一个特别重要的技巧，就是将输入进行倒排。通过实验，发现这样可以大大提升结果。从直观分析来说，这个技巧能起到作用的原因是将输出的词和输出的词在神经网络上的距离拉近了。例如在figure1里面，原来A到X的距离为4个单元，通过倒排A到X的距离变为2个单元。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在工程实现上，作者采用了8个GPU，每一层的LSTM用一个GPU计算，在前一个GPU计算完结果后会传递到下一个GPU来计算。另外4个GPU则用于softmax的并行计算。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-&amp;#23454;&amp;#39564;&amp;#32467;&amp;#26524;&quot;&gt;3. &amp;#23454;&amp;#39564;&amp;#32467;&amp;#26524;&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-&amp;#23454;&amp;#39564;&amp;#32467;&amp;#26524;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;实验结果如下面的table2所示，可以看出端到端的Seq2Seq模型在翻译任务上取到了比较不错的结果。提高Beam search的size和增加集成模型的各种都有助于提高模型精度。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq2.PNG&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;另外，作者还分析了编码器对于语序和语态的表现能力。从图中可以看出模型对于语序非常敏感，但是不能很好的区分主动语态和被动语态。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq3.PNG&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;最后作者比较了LSTM的seq2seq模型和baseline(统计模型)在各种句子长度下的表现，发现LSTM在各种句子长度下都能有比较好的效果。
另外figure3的右图指出，在句子里常用词比较多的情况下，LSTM的表现会更好。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/seq2seq4.PNG&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-&amp;#24635;&amp;#32467;&quot;&gt;4. &amp;#24635;&amp;#32467;&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-&amp;#24635;&amp;#32467;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文提出了一种处理不固定长序列问题的端到端的方法。大大简化了以往统计建模的时间，并且精度上也达到了一个不错的级别。个人感觉，在建模上，大家可以都能想到seq2seq这种结构。但是一些trick(比如倒排输入的句子)不太容易想到，而且在工程实现上，利用多个GPU并行计算LSTM在当时也不容易做到&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &lt;a href=&quot;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &lt;a href=&quot;https://blog.csdn.net/allocator/article/details/79657792&quot;&gt;自然语言处理——BLEU详解以及简单的代码实现&lt;/a&gt;&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">Evaluation methods for unsupervised word embeddings</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings.html" rel="alternate" type="text/html" title="Evaluation methods for unsupervised word embeddings" /><published>2020-07-04T00:00:00-05:00</published><updated>2020-07-04T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-04-Evaluation-methods-for-unsupervised-word-embeddings.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文比较了各种衡量词向量的方法，并提出了一种新的评测词向量的方法。本文主要有以下贡献&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分析了不同评判标准间的关系，表明了生成词向量的方式要和特殊任务相关联&lt;/li&gt;
&lt;li&gt;提出了一种通过人为评分方式衡量直接衡量单个词向量的方法&lt;/li&gt;
&lt;li&gt;提出了选择词向量（用于评价）时要考虑到选择不同词频，词性，词义的向量。保证数据的多样性&lt;/li&gt;
&lt;li&gt;本文还发现了词向量包含着词频信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;需要注意的是这篇文章的目的不是去比较词向量的好坏，而是去研究评判词向量方法的差别。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-Embeeding&amp;#30340;&amp;#20934;&amp;#22791;&quot;&gt;1. Embeeding&amp;#30340;&amp;#20934;&amp;#22791;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Embeeding&amp;#30340;&amp;#20934;&amp;#22791;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文准备了以下六种生成词向量的方式用于评判：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于概率预测的embeeding&lt;ul&gt;
&lt;li&gt;CBOW model of word2vec (Mikolov et al 2013a)&lt;/li&gt;
&lt;li&gt;C&amp;amp;W embeddings (Collobert et al. 2011)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于反应语料中的词汇的同现关系&lt;ul&gt;
&lt;li&gt;Hellinger PCA (Lebret and COllobert 2014)&lt;/li&gt;
&lt;li&gt;GloVe (Pennington et al., 2014)&lt;/li&gt;
&lt;li&gt;TSCCA (Dhillon et al., 2012)&lt;/li&gt;
&lt;li&gt;Sparse Random Projections (Li et al., 2006)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于C&amp;amp;W的词向，因为只有基于2007年的维基百科的。所以本文选取了2008-03-01日的维基百科来训练其余5中词向量。这里，所有词向量的维度为50，总共的词典大小为103647&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Evaluation&quot;&gt;2. Evaluation&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Evaluation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;评价词向量主要有两种方式，一种是内部评价（intrinsic evaluation），另一种是外部评价（extrinsic evaluation）。&lt;/p&gt;
&lt;p&gt;内部评价指的是用词的词性，相关性等内部固有关系来评价生成的词向量的好坏。
外部评价指的是用生成的词向量去作为下游任务的输入，看哪种词向量可以更好的实现下游任务。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#20869;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;intrinsic-evaluation&amp;#65289;&quot;&gt;&amp;#20869;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;intrinsic evaluation&amp;#65289;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20869;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;intrinsic-evaluation&amp;#65289;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;对于内部评价，本文采用的绝对的内部评价（absolute intrinsic evaluation）和相对的内部评价（comparative intrinsic evaluation），绝对内部评价有以下方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relatedness：比较生成的词向量的词于词之间的余弦相似度和人类评价的相似度的关系&lt;/li&gt;
&lt;li&gt;Analogy：对于一个y，去找到一个x，使得x:y的关系要和a:b的关系一样&lt;/li&gt;
&lt;li&gt;Categorization：把生成的词向量做聚类，看聚类是否准确&lt;/li&gt;
&lt;li&gt;Selectional preference：确定一个词是某个动词的主语还是宾语&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;评价结果如下,可以看出，绝大多数任务中，CBOW表现最好。但是个别任务里，其他词向量更好&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding1.PNG&quot; alt=&quot;Embedding1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在相对内部评价中，用户直接来判断词向量的好坏。作者的具体做法如下，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选取了词频，词性和词义不同的100个单词（选择10种类别的词，每种类别里有一个形容词，一个动词，4个名词，4个动词）&lt;/li&gt;
&lt;li&gt;找出每个词的n nearest neighbors, 选取rank为1，5，50的neighbor。所以对于6中词向量，对于每一个词，我们分为计算出rank为1，5，50的neighbor。&lt;/li&gt;
&lt;li&gt;让人类来分别评价6中词向量中，rank1，5，50的neighbor里哪个于选定词最近。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结果如下,同样可以看出，没有一种词向量是在所有任务中都表现最好的&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding2.PNG&quot; alt=&quot;Embedding2&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;在相似度（relatedness）的比较中，我们对于任意一个单词，我们只找了一个相近的单词，这并不理想（因为每个单词都有很多近义词）。所以作者提出了一种新的衡量方式：Coherence。对于每一个单词，事先选出两个近义词和一个不相关的词，看用生成的词向量能否辨别出不想关的词。&lt;/p&gt;
&lt;p&gt;结果如下,可以看出不同词向量的生成方法，对于不同词频的单词，所得到的结果是不同的&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding3.PNG&quot; alt=&quot;Embedding3&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#22806;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;Extrinsic-evaluation&amp;#65289;&quot;&gt;&amp;#22806;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;Extrinsic evaluation&amp;#65289;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#22806;&amp;#37096;&amp;#35780;&amp;#20215;&amp;#65288;Extrinsic-evaluation&amp;#65289;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;外部评价主要用来测量词向量对于下游任务的贡献。本文选取了以下两种下游任务来评判&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Noun phrase chunking：名词分块&lt;/li&gt;
&lt;li&gt;Sentiment classification：情感分类&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结果如下，对于下游任务，同样的，没有一种词向量可以在所有下游任务中都表现最好，所以对于不同下游任务，我们应该尝试不同词向量的表示&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding4.PNG&quot; alt=&quot;Embedding4&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Frequency-information&quot;&gt;3. Frequency information&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Frequency-information&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;最后，作者通过以下两种实验发现了词向量里面包含词频信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用词向量来预测单词在语料中词频&lt;/li&gt;
&lt;li&gt;对于所有在WordSim-353数据集的单词，研究其K=1000 nearest neighbors和他们在语料中词频的大小排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结果如下,可以看出，我们可以通过词向量来较好的预测单词的词频，其中GloVe和CCA中包含了较多的词频信息。另外单词的词频于其在语料库里的词频排名也有很强的相关性&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/Embedding5.PNG&quot; alt=&quot;Embedding5&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-&amp;#24605;&amp;#32771;&quot;&gt;4. &amp;#24605;&amp;#32771;&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-&amp;#24605;&amp;#32771;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;通过本文，我们发现没有任何一种词向量可以在所有任务中都表现的最好，所以每个单词应该不存在一种绝对正确的词向量。那么，词向量是否是用来表示单词的最好方式呢，我对此表示疑问。以后很有可能会发现一种新的表示单词的方式。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">文本预处理方法汇总</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/03/Text-Preprocessing.html" rel="alternate" type="text/html" title="文本预处理方法汇总" /><published>2020-07-03T00:00:00-05:00</published><updated>2020-07-03T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/07/03/Text-Preprocessing</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/07/03/Text-Preprocessing.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-03-Text-Preprocessing.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;本文汇总各种文本预处理的方法，皆在方便自己快速查找。
original link is here &lt;a href=&quot;https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&quot;&gt;https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;all-to-upper-case-or-lowwer-case&quot;&gt;all to upper case or lowwer case&lt;a class=&quot;anchor-link&quot; href=&quot;#all-to-upper-case-or-lowwer-case&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;AbcdEfG&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;&amp;#39;abcdefg&amp;#39;&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;replace-numbers-or-remove-numbers&quot;&gt;replace numbers or remove numbers&lt;a class=&quot;anchor-link&quot; href=&quot;#replace-numbers-or-remove-numbers&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;\d+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Box A contains  red and  white balls, while Box B contains  red and  blue balls.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Remove-Punctuation&quot;&gt;Remove Punctuation&lt;a class=&quot;anchor-link&quot; href=&quot;#Remove-Punctuation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;string&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;This &amp;amp;is [an] example? &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{of}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; string. with.? punctuation!!!!&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;punctuation_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;punctuation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;translate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;punctuation_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;This is an example of string with punctuation
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Remove-Whitespace&quot;&gt;Remove Whitespace&lt;a class=&quot;anchor-link&quot; href=&quot;#Remove-Whitespace&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;   This has a lot whitespace    &amp;quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;This has a lot whitespace
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Tokenization&quot;&gt;Tokenization&lt;a class=&quot;anchor-link&quot; href=&quot;#Tokenization&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/3220/1*ffMYw8aujrmyxfA55Zm3Jg.jpeg&quot; alt=&quot;tokenization&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.tokenize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WhitespaceTokenizer&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WhitespaceTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;I love you&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;[&amp;#39;I&amp;#39;, &amp;#39;love&amp;#39;, &amp;#39;you&amp;#39;]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Remove-Stop-words&quot;&gt;Remove Stop words&lt;a class=&quot;anchor-link&quot; href=&quot;#Remove-Stop-words&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.corpus&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stopwords&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;NLTK is a leading platform for building Python programs to work with human language data.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;[&amp;#39;NLTK&amp;#39;, &amp;#39;leading&amp;#39;, &amp;#39;platform&amp;#39;, &amp;#39;building&amp;#39;, &amp;#39;Python&amp;#39;, &amp;#39;programs&amp;#39;, &amp;#39;work&amp;#39;, &amp;#39;human&amp;#39;, &amp;#39;language&amp;#39;, &amp;#39;data.&amp;#39;]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.feature_extraction.stop_words&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ENGLISH_STOP_WORDS&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ENGLISH_STOP_WORDS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;frozenset({&amp;#39;together&amp;#39;, &amp;#39;seemed&amp;#39;, &amp;#39;she&amp;#39;, &amp;#39;hers&amp;#39;, &amp;#39;ie&amp;#39;, &amp;#39;may&amp;#39;, &amp;#39;becoming&amp;#39;, &amp;#39;though&amp;#39;, &amp;#39;everything&amp;#39;, &amp;#39;only&amp;#39;, &amp;#39;somewhere&amp;#39;, &amp;#39;at&amp;#39;, &amp;#39;one&amp;#39;, &amp;#39;very&amp;#39;, &amp;#39;few&amp;#39;, &amp;#39;many&amp;#39;, &amp;#39;whither&amp;#39;, &amp;#39;my&amp;#39;, &amp;#39;onto&amp;#39;, &amp;#39;now&amp;#39;, &amp;#39;keep&amp;#39;, &amp;#39;mill&amp;#39;, &amp;#39;this&amp;#39;, &amp;#39;than&amp;#39;, &amp;#39;once&amp;#39;, &amp;#39;seems&amp;#39;, &amp;#39;might&amp;#39;, &amp;#39;please&amp;#39;, &amp;#39;these&amp;#39;, &amp;#39;among&amp;#39;, &amp;#39;hence&amp;#39;, &amp;#39;thus&amp;#39;, &amp;#39;something&amp;#39;, &amp;#39;rather&amp;#39;, &amp;#39;how&amp;#39;, &amp;#39;whereas&amp;#39;, &amp;#39;whence&amp;#39;, &amp;#39;everywhere&amp;#39;, &amp;#39;last&amp;#39;, &amp;#39;anyone&amp;#39;, &amp;#39;never&amp;#39;, &amp;#39;somehow&amp;#39;, &amp;#39;another&amp;#39;, &amp;#39;herself&amp;#39;, &amp;#39;i&amp;#39;, &amp;#39;detail&amp;#39;, &amp;#39;two&amp;#39;, &amp;#39;elsewhere&amp;#39;, &amp;#39;give&amp;#39;, &amp;#39;nowhere&amp;#39;, &amp;#39;myself&amp;#39;, &amp;#39;me&amp;#39;, &amp;#39;some&amp;#39;, &amp;#39;of&amp;#39;, &amp;#39;everyone&amp;#39;, &amp;#39;first&amp;#39;, &amp;#39;yourselves&amp;#39;, &amp;#39;himself&amp;#39;, &amp;#39;meanwhile&amp;#39;, &amp;#39;serious&amp;#39;, &amp;#39;found&amp;#39;, &amp;#39;hereafter&amp;#39;, &amp;#39;much&amp;#39;, &amp;#39;becomes&amp;#39;, &amp;#39;nobody&amp;#39;, &amp;#39;thin&amp;#39;, &amp;#39;namely&amp;#39;, &amp;#39;find&amp;#39;, &amp;#39;indeed&amp;#39;, &amp;#39;thru&amp;#39;, &amp;#39;those&amp;#39;, &amp;#39;no&amp;#39;, &amp;#39;noone&amp;#39;, &amp;#39;both&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;hasnt&amp;#39;, &amp;#39;own&amp;#39;, &amp;#39;not&amp;#39;, &amp;#39;amoungst&amp;#39;, &amp;#39;empty&amp;#39;, &amp;#39;then&amp;#39;, &amp;#39;their&amp;#39;, &amp;#39;again&amp;#39;, &amp;#39;further&amp;#39;, &amp;#39;itself&amp;#39;, &amp;#39;most&amp;#39;, &amp;#39;hereby&amp;#39;, &amp;#39;up&amp;#39;, &amp;#39;wherein&amp;#39;, &amp;#39;to&amp;#39;, &amp;#39;thereupon&amp;#39;, &amp;#39;across&amp;#39;, &amp;#39;on&amp;#39;, &amp;#39;along&amp;#39;, &amp;#39;except&amp;#39;, &amp;#39;done&amp;#39;, &amp;#39;anyway&amp;#39;, &amp;#39;had&amp;#39;, &amp;#39;go&amp;#39;, &amp;#39;any&amp;#39;, &amp;#39;will&amp;#39;, &amp;#39;often&amp;#39;, &amp;#39;upon&amp;#39;, &amp;#39;three&amp;#39;, &amp;#39;fire&amp;#39;, &amp;#39;neither&amp;#39;, &amp;#39;anyhow&amp;#39;, &amp;#39;either&amp;#39;, &amp;#39;there&amp;#39;, &amp;#39;forty&amp;#39;, &amp;#39;re&amp;#39;, &amp;#39;per&amp;#39;, &amp;#39;formerly&amp;#39;, &amp;#39;beside&amp;#39;, &amp;#39;seeming&amp;#39;, &amp;#39;inc&amp;#39;, &amp;#39;amount&amp;#39;, &amp;#39;un&amp;#39;, &amp;#39;could&amp;#39;, &amp;#39;out&amp;#39;, &amp;#39;against&amp;#39;, &amp;#39;twelve&amp;#39;, &amp;#39;system&amp;#39;, &amp;#39;mostly&amp;#39;, &amp;#39;down&amp;#39;, &amp;#39;other&amp;#39;, &amp;#39;between&amp;#39;, &amp;#39;thereafter&amp;#39;, &amp;#39;below&amp;#39;, &amp;#39;full&amp;#39;, &amp;#39;our&amp;#39;, &amp;#39;would&amp;#39;, &amp;#39;anything&amp;#39;, &amp;#39;are&amp;#39;, &amp;#39;almost&amp;#39;, &amp;#39;but&amp;#39;, &amp;#39;bottom&amp;#39;, &amp;#39;your&amp;#39;, &amp;#39;made&amp;#39;, &amp;#39;see&amp;#39;, &amp;#39;until&amp;#39;, &amp;#39;eg&amp;#39;, &amp;#39;beforehand&amp;#39;, &amp;#39;as&amp;#39;, &amp;#39;therefore&amp;#39;, &amp;#39;cannot&amp;#39;, &amp;#39;enough&amp;#39;, &amp;#39;what&amp;#39;, &amp;#39;became&amp;#39;, &amp;#39;con&amp;#39;, &amp;#39;through&amp;#39;, &amp;#39;front&amp;#39;, &amp;#39;six&amp;#39;, &amp;#39;from&amp;#39;, &amp;#39;all&amp;#39;, &amp;#39;the&amp;#39;, &amp;#39;put&amp;#39;, &amp;#39;someone&amp;#39;, &amp;#39;throughout&amp;#39;, &amp;#39;former&amp;#39;, &amp;#39;has&amp;#39;, &amp;#39;still&amp;#39;, &amp;#39;due&amp;#39;, &amp;#39;next&amp;#39;, &amp;#39;fifteen&amp;#39;, &amp;#39;off&amp;#39;, &amp;#39;and&amp;#39;, &amp;#39;cant&amp;#39;, &amp;#39;alone&amp;#39;, &amp;#39;amongst&amp;#39;, &amp;#39;besides&amp;#39;, &amp;#39;side&amp;#39;, &amp;#39;about&amp;#39;, &amp;#39;we&amp;#39;, &amp;#39;he&amp;#39;, &amp;#39;eleven&amp;#39;, &amp;#39;always&amp;#39;, &amp;#39;was&amp;#39;, &amp;#39;whatever&amp;#39;, &amp;#39;none&amp;#39;, &amp;#39;whenever&amp;#39;, &amp;#39;whole&amp;#39;, &amp;#39;where&amp;#39;, &amp;#39;her&amp;#39;, &amp;#39;above&amp;#39;, &amp;#39;also&amp;#39;, &amp;#39;ourselves&amp;#39;, &amp;#39;four&amp;#39;, &amp;#39;top&amp;#39;, &amp;#39;fill&amp;#39;, &amp;#39;although&amp;#39;, &amp;#39;which&amp;#39;, &amp;#39;move&amp;#39;, &amp;#39;sixty&amp;#39;, &amp;#39;thence&amp;#39;, &amp;#39;were&amp;#39;, &amp;#39;nothing&amp;#39;, &amp;#39;bill&amp;#39;, &amp;#39;however&amp;#39;, &amp;#39;you&amp;#39;, &amp;#39;hundred&amp;#39;, &amp;#39;same&amp;#39;, &amp;#39;must&amp;#39;, &amp;#39;ltd&amp;#39;, &amp;#39;been&amp;#39;, &amp;#39;they&amp;#39;, &amp;#39;whereupon&amp;#39;, &amp;#39;sincere&amp;#39;, &amp;#39;name&amp;#39;, &amp;#39;because&amp;#39;, &amp;#39;hereupon&amp;#39;, &amp;#39;others&amp;#39;, &amp;#39;who&amp;#39;, &amp;#39;cry&amp;#39;, &amp;#39;his&amp;#39;, &amp;#39;while&amp;#39;, &amp;#39;if&amp;#39;, &amp;#39;too&amp;#39;, &amp;#39;since&amp;#39;, &amp;#39;or&amp;#39;, &amp;#39;sometimes&amp;#39;, &amp;#39;therein&amp;#39;, &amp;#39;without&amp;#39;, &amp;#39;ten&amp;#39;, &amp;#39;eight&amp;#39;, &amp;#39;via&amp;#39;, &amp;#39;five&amp;#39;, &amp;#39;into&amp;#39;, &amp;#39;ours&amp;#39;, &amp;#39;co&amp;#39;, &amp;#39;yours&amp;#39;, &amp;#39;an&amp;#39;, &amp;#39;themselves&amp;#39;, &amp;#39;us&amp;#39;, &amp;#39;for&amp;#39;, &amp;#39;thick&amp;#39;, &amp;#39;thereby&amp;#39;, &amp;#39;so&amp;#39;, &amp;#39;more&amp;#39;, &amp;#39;under&amp;#39;, &amp;#39;am&amp;#39;, &amp;#39;by&amp;#39;, &amp;#39;less&amp;#39;, &amp;#39;ever&amp;#39;, &amp;#39;otherwise&amp;#39;, &amp;#39;whoever&amp;#39;, &amp;#39;nine&amp;#39;, &amp;#39;even&amp;#39;, &amp;#39;wherever&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;yourself&amp;#39;, &amp;#39;herein&amp;#39;, &amp;#39;every&amp;#39;, &amp;#39;part&amp;#39;, &amp;#39;each&amp;#39;, &amp;#39;already&amp;#39;, &amp;#39;such&amp;#39;, &amp;#39;in&amp;#39;, &amp;#39;afterwards&amp;#39;, &amp;#39;be&amp;#39;, &amp;#39;least&amp;#39;, &amp;#39;why&amp;#39;, &amp;#39;anywhere&amp;#39;, &amp;#39;with&amp;#39;, &amp;#39;them&amp;#39;, &amp;#39;perhaps&amp;#39;, &amp;#39;latter&amp;#39;, &amp;#39;seem&amp;#39;, &amp;#39;back&amp;#39;, &amp;#39;during&amp;#39;, &amp;#39;can&amp;#39;, &amp;#39;else&amp;#39;, &amp;#39;being&amp;#39;, &amp;#39;over&amp;#39;, &amp;#39;whose&amp;#39;, &amp;#39;within&amp;#39;, &amp;#39;moreover&amp;#39;, &amp;#39;whereby&amp;#39;, &amp;#39;fifty&amp;#39;, &amp;#39;mine&amp;#39;, &amp;#39;several&amp;#39;, &amp;#39;get&amp;#39;, &amp;#39;its&amp;#39;, &amp;#39;well&amp;#39;, &amp;#39;take&amp;#39;, &amp;#39;whom&amp;#39;, &amp;#39;after&amp;#39;, &amp;#39;it&amp;#39;, &amp;#39;third&amp;#39;, &amp;#39;describe&amp;#39;, &amp;#39;whereafter&amp;#39;, &amp;#39;nor&amp;#39;, &amp;#39;that&amp;#39;, &amp;#39;before&amp;#39;, &amp;#39;interest&amp;#39;, &amp;#39;when&amp;#39;, &amp;#39;call&amp;#39;, &amp;#39;sometime&amp;#39;, &amp;#39;nevertheless&amp;#39;, &amp;#39;toward&amp;#39;, &amp;#39;show&amp;#39;, &amp;#39;latterly&amp;#39;, &amp;#39;twenty&amp;#39;, &amp;#39;yet&amp;#39;, &amp;#39;couldnt&amp;#39;, &amp;#39;have&amp;#39;, &amp;#39;him&amp;#39;, &amp;#39;around&amp;#39;, &amp;#39;become&amp;#39;, &amp;#39;do&amp;#39;, &amp;#39;behind&amp;#39;, &amp;#39;beyond&amp;#39;, &amp;#39;towards&amp;#39;, &amp;#39;here&amp;#39;, &amp;#39;de&amp;#39;, &amp;#39;should&amp;#39;, &amp;#39;whether&amp;#39;, &amp;#39;etc&amp;#39;})
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Stemming-&amp;amp;-Remove-sparse-terms-and-particular-words&quot;&gt;Stemming &amp;amp; Remove sparse terms and particular words&lt;a class=&quot;anchor-link&quot; href=&quot;#Stemming-&amp;amp;-Remove-sparse-terms-and-particular-words&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/3492/1*JpOXoNSFkZ0sjqPYT2U4cA.jpeg&quot; alt=&quot;Stemming&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.stem&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PorterStemmer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.tokenize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PorterStemmer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;There are several types of stemming algorithms.&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stemmer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;there
are
sever
type
of
stem
algorithm
.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Lemmatization&quot;&gt;Lemmatization&lt;a class=&quot;anchor-link&quot; href=&quot;#Lemmatization&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.stem&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WordNetLemmatizer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk.tokenize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lemmatizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WordNetLemmatizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;been had done languages cities mice&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemmatizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lemmatize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;been
had
done
language
city
mouse
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-of-speech-tagging-(POS)&quot;&gt;Part of speech tagging (POS)&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-of-speech-tagging-(POS)&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_tag&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Parts of speech examples: an article, to write, interesting, easily, and, of&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;[(&amp;#39;Parts&amp;#39;, &amp;#39;NNS&amp;#39;), (&amp;#39;of&amp;#39;, &amp;#39;IN&amp;#39;), (&amp;#39;speech&amp;#39;, &amp;#39;NN&amp;#39;), (&amp;#39;examples&amp;#39;, &amp;#39;NNS&amp;#39;), (&amp;#39;:&amp;#39;, &amp;#39;:&amp;#39;), (&amp;#39;an&amp;#39;, &amp;#39;DT&amp;#39;), (&amp;#39;article&amp;#39;, &amp;#39;NN&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;to&amp;#39;, &amp;#39;TO&amp;#39;), (&amp;#39;write&amp;#39;, &amp;#39;VB&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;interesting&amp;#39;, &amp;#39;VBG&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;easily&amp;#39;, &amp;#39;RB&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;and&amp;#39;, &amp;#39;CC&amp;#39;), (&amp;#39;,&amp;#39;, &amp;#39;,&amp;#39;), (&amp;#39;of&amp;#39;, &amp;#39;IN&amp;#39;)]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Chunking-(shallow-parsing)&quot;&gt;Chunking (shallow parsing)&lt;a class=&quot;anchor-link&quot; href=&quot;#Chunking-(shallow-parsing)&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) [23]. Chunking tools: NLTK, TreeTagger chunker, Apache OpenNLP, General Architecture for Text Engineering (GATE), FreeLing.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry><entry><title type="html">From Word Embeddings To Document Distances详解</title><link href="https://xubujie.github.io/DataScienceBlog/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances.html" rel="alternate" type="text/html" title="From Word Embeddings To Document Distances详解" /><published>2020-06-11T00:00:00-05:00</published><updated>2020-06-11T00:00:00-05:00</updated><id>https://xubujie.github.io/DataScienceBlog/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances</id><content type="html" xml:base="https://xubujie.github.io/DataScienceBlog/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-11-From-Word-Embeddings-To-Document-Distances.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-&amp;#31616;&amp;#20171;&quot;&gt;1. &amp;#31616;&amp;#20171;&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-&amp;#31616;&amp;#20171;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;《From Word Embeddings To Document Distances》这篇文章，发表于2014年。作者在word2vec的基础上提出了一种衡量文章相似度的尺度，Word Mover's Distance(WDM)。 WDM用来描述两篇文章的词向量之间的距离。这里的“距离”作者用了“旅行距离”来描述，意思是从一篇文章的词向量转换到另一篇文章的词向量的最短距离。在此距离的基础上，作者提出了衡量文章相似度的算法。从实验可以看出，该方法优于当时的SOTA。&lt;/p&gt;
&lt;p&gt;下文会按照作者的思路来详细介绍WDM的定义和文章相似度的计算方法（站在作者角度来想）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在计算文章相似度的问题中，前人的方法只考虑词频，没有考虑文章的意义，把word2vec用到文章相似度的评价里会不会有所提高呢？&lt;/li&gt;
&lt;li&gt;那么我来定义了一种新的计算文章相似度的方法WMD。&lt;/li&gt;
&lt;li&gt;可是WMD的计算复杂度太高。那我提出计算下限的方法WCD和RWMD。&lt;/li&gt;
&lt;li&gt;有了WCD和RWMD，然后通过Prefetch和Prune来找到一个文章的k nearest neighbors。&lt;/li&gt;
&lt;li&gt;看看我的实验结果多牛叉。超过了当时的SOTA。&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-&amp;#35770;&amp;#25991;&amp;#35814;&amp;#35299;&quot;&gt;2. &amp;#35770;&amp;#25991;&amp;#35814;&amp;#35299;&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-&amp;#35770;&amp;#25991;&amp;#35814;&amp;#35299;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;&amp;#19968;&amp;#12289;Word2Vec-Embedding&quot;&gt;&amp;#19968;&amp;#12289;Word2Vec Embedding&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#19968;&amp;#12289;Word2Vec-Embedding&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;在2013年，Mikolov et al提出了word2vec，这个在当时引起了很大反响。该研究用向量来表示每个单词，从数学上更好的表示了单词的意义。传统上表示文章，大多数都是用BOW或者TFIDF的手法，这些手法对于词的意义有欠考虑，所以在衡量文章相似度上，往往停留在单词一致性的表层上，文章意思的相似性得不到很大的表现。作者关注到word2vec的发展，然后将该方法应用到了文章相似度的表达上。这里我们首先介绍以下word2vec。&lt;/p&gt;
&lt;p&gt;简单来说，word2vec是一种通过学习神经网络来寻找词向量表示的一种方法。具体来说word2vec的skip-gram模型是通过构建一个单层神经网络（input layer, projection layer, output layer)来预测每个单词的相邻单词。通过学习该神经网络，得到的projection layer即为每个单词的词向量。训练目标是使每个单词的相邻单词的出现概率最大化。该概率可以用下式表示
$$
\frac{1}{T}\sum_{t=1}^T\sum_{j \in nb(t)}logp(w_j|w_t) \tag{1}
$$&lt;/p&gt;
&lt;p&gt;这里T为单词数，nb(t)表示单词$w_t$的相邻单词，$p(w_j|w_t)$用hierarchical softmax来提高训练速度。&lt;/p&gt;
&lt;h3 id=&quot;&amp;#20108;&amp;#12289;Word-Move's-Distance&quot;&gt;&amp;#20108;&amp;#12289;Word Move's Distance&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20108;&amp;#12289;Word-Move's-Distance&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;首先我们用词向量来定义Word travel cost。假设$x_i, x_j$表示word $i$和word $j$的词向量。我们用$c(i, j) = ||x_i - x_j||_2$来表示从一个词到另一个词的&quot;旅行距离&quot;（Word travel cost）。有了词于词之间的距离，接下来我们来定义文章间距离。设$d$, $d'$为两篇文章的nBow (normalized bag of words)表示。令$T_{ij}, T \in R^{n\times n}$来表示文章d的词i到文章d'的距离。我们令词i到文章d'的所有词的距离之和为$\sum_jT_{ij} = d_i$, 反之文章d'的词j到文章d的所有词之和为$\sum_iT_{ij}=d'_j$。最终，我们定义两个文章的距离为从一篇文章d到另一篇文章d‘距离的加权累加的最小值。用数学描述为
$$
\min_{T&amp;gt;=0}\sum_{i,j=1}^nT_{ij}c(i,j)　\\
subject \ to: \sum_{j=1}^nT_{i,j} = d_i, \forall_i \in \{1,...,n\} \\
\sum_{i=1}^nT_{i,j} = d'_j, \forall_j \in \{1,...,n\} \tag{2}
$$
所以，求解文章与文章之间的WMD转化为了一个最优化问题。&lt;/p&gt;
&lt;p&gt;通过下图简要举例介绍词数相同时文章间距离和词数不同时文章距离的情况。在上半部分，去除了（the, to ,in, a）等等stopwords之后，D0,D1,D2都是由4个不同的词构成的。所以，所有的词的$d_i=0.25$，这里的箭头表示的是$T_{ij}c(i,j)$, 由于词向量空间上，比起band，Obama离President更近，这里的分数也很好的反应了该结果。在图的下半部分，我们可以看出，当词数不同时，一个词可能会映射到多个相似的词。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/WMD1.png&quot; alt=&quot;WMD1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#19977;&amp;#12289;&amp;#31867;&amp;#27604;&amp;#36816;&amp;#36755;&amp;#38382;&amp;#39064;&quot;&gt;&amp;#19977;&amp;#12289;&amp;#31867;&amp;#27604;&amp;#36816;&amp;#36755;&amp;#38382;&amp;#39064;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#19977;&amp;#12289;&amp;#31867;&amp;#27604;&amp;#36816;&amp;#36755;&amp;#38382;&amp;#39064;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;运输问题的典型情况是研究单一品种物质的运输调度问题：设某种物品有m个产地$A_1，A_2，···，A_m$，各产地的产量分别是$a_1，a_2，···，a_m$,有n个销地$B_1，B_2，···，B_n$，各个销地的销量分别为$b_1，b_2，···，b_n$。假定从产地$A_i(i=1,2,···,m)$向销地$B_j(j=1,2,···,n)$运输单位物品的运价为$c_{ij}$，怎么调运这些物品才能使总运费最小？可以看出，文章的WMD计算和运输问题是完全对应的。这里产地$A_1，A_2，···，A_m$对应于文章A的单词，$a_1，a_2，···，a_m$为单词的词频。同理$B_1，B_2，···，B_n$为文章B的单词，$b_1，b_2，···，b_n$为对应的词频。运价$c_{ij}$对应于文章A的单词和文章B的单词间的欧拉距离。运输问题可以描述成以下线性规划问题。&lt;/p&gt;
$$
\min_z = \sum_{i=1}^m\sum_{j=1}^nc_{ij}x_{ij} \\
subject \ to: \sum_{j=1}^nx_{i,j} = a_i, \forall_i \in \{1,...,n\} \\
\sum_{i=1}^nx_{i,j} = b_j, \forall_j \in \{1,...,n\}  \\
x_{ij} \geq 0 \tag{3}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#22235;&amp;#12289;Fast-Distance-Computation&quot;&gt;&amp;#22235;&amp;#12289;Fast Distance Computation&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#22235;&amp;#12289;Fast-Distance-Computation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;解决上述WMD最优化问题的复杂度时$O(p^3\log p)$, p为去重后单词数。可以想象，当文章有很多单词时，计算WMD会变的非常困难。为此，作者提出了两种求解WMD下限的方法&lt;/p&gt;
&lt;h5 id=&quot;WCD&amp;#65306;&amp;#36890;&amp;#36807;&amp;#19977;&amp;#35282;&amp;#19981;&amp;#31561;&amp;#24335;$||x+y||-\leq-||x||-+-||y||$&amp;#65292;&amp;#25105;&amp;#20204;&amp;#21487;&amp;#20197;&amp;#31616;&amp;#21333;&amp;#30340;&amp;#24471;&amp;#21040;&quot;&gt;WCD&amp;#65306;&amp;#36890;&amp;#36807;&amp;#19977;&amp;#35282;&amp;#19981;&amp;#31561;&amp;#24335;$||x+y|| \leq ||x|| + ||y||$&amp;#65292;&amp;#25105;&amp;#20204;&amp;#21487;&amp;#20197;&amp;#31616;&amp;#21333;&amp;#30340;&amp;#24471;&amp;#21040;&lt;a class=&quot;anchor-link&quot; href=&quot;#WCD&amp;#65306;&amp;#36890;&amp;#36807;&amp;#19977;&amp;#35282;&amp;#19981;&amp;#31561;&amp;#24335;$||x+y||-\leq-||x||-+-||y||$&amp;#65292;&amp;#25105;&amp;#20204;&amp;#21487;&amp;#20197;&amp;#31616;&amp;#21333;&amp;#30340;&amp;#24471;&amp;#21040;&quot;&gt; &lt;/a&gt;&lt;/h5&gt;$$
\sum_{i,j=1}^nT_{ij}c(i,j) = \sum_{i,j=1}^nT_{ij}||x_i-x'_j||_2 \\
= \sum_{i,j=1}||T_{ij}(x_i-x'_j)||_2 \geq ||\sum_{i,j=1}^nT_{ij}(x_i-x'_j)||_2 \\
= ||\sum_{i=1}^n(\sum_{j=1}^nT_{ij})x_i-\sum_{j=1}^n(\sum_{j=1}^nT_{ij})x'_j||_2 \\
= ||\sum_{i=1}^nd_ix_i - \sum_{j=1}^nd'_jx'_j||_2 \tag{4}
$$&lt;p&gt;作者将这个距离称为Word Centroid Distance（WCD），可以看出计算这个WMD的下限WCD非常快，算法复杂度是O(dp), d为词向量的维度，p为去重后的单词数。在寻找某篇文章的k个最相似的文章时，WCD可以用于寻找有利的候补，从而提高算法效率。&lt;/p&gt;
&lt;h5 id=&quot;RWMD&amp;#65306;&amp;#34429;&amp;#28982;WCD&amp;#24456;&amp;#23481;&amp;#26131;&amp;#35745;&amp;#31639;&amp;#65292;&amp;#20294;&amp;#26159;&amp;#22240;&amp;#20026;&amp;#20182;&amp;#32473;&amp;#20986;&amp;#30340;&amp;#19979;&amp;#38480;&amp;#22826;&amp;#20302;&amp;#65292;&amp;#20316;&amp;#32773;&amp;#22312;&amp;#36825;&amp;#37324;&amp;#25552;&amp;#20986;&amp;#20102;&amp;#19968;&amp;#31181;&amp;#26356;&amp;#21152;&amp;#36924;&amp;#36817;&amp;#30495;&amp;#23454;&amp;#20540;&amp;#30340;&amp;#19979;&amp;#38480;Relaxed-word-moving-distance&amp;#12290;&quot;&gt;RWMD&amp;#65306;&amp;#34429;&amp;#28982;WCD&amp;#24456;&amp;#23481;&amp;#26131;&amp;#35745;&amp;#31639;&amp;#65292;&amp;#20294;&amp;#26159;&amp;#22240;&amp;#20026;&amp;#20182;&amp;#32473;&amp;#20986;&amp;#30340;&amp;#19979;&amp;#38480;&amp;#22826;&amp;#20302;&amp;#65292;&amp;#20316;&amp;#32773;&amp;#22312;&amp;#36825;&amp;#37324;&amp;#25552;&amp;#20986;&amp;#20102;&amp;#19968;&amp;#31181;&amp;#26356;&amp;#21152;&amp;#36924;&amp;#36817;&amp;#30495;&amp;#23454;&amp;#20540;&amp;#30340;&amp;#19979;&amp;#38480;Relaxed word moving distance&amp;#12290;&lt;a class=&quot;anchor-link&quot; href=&quot;#RWMD&amp;#65306;&amp;#34429;&amp;#28982;WCD&amp;#24456;&amp;#23481;&amp;#26131;&amp;#35745;&amp;#31639;&amp;#65292;&amp;#20294;&amp;#26159;&amp;#22240;&amp;#20026;&amp;#20182;&amp;#32473;&amp;#20986;&amp;#30340;&amp;#19979;&amp;#38480;&amp;#22826;&amp;#20302;&amp;#65292;&amp;#20316;&amp;#32773;&amp;#22312;&amp;#36825;&amp;#37324;&amp;#25552;&amp;#20986;&amp;#20102;&amp;#19968;&amp;#31181;&amp;#26356;&amp;#21152;&amp;#36924;&amp;#36817;&amp;#30495;&amp;#23454;&amp;#20540;&amp;#30340;&amp;#19979;&amp;#38480;Relaxed-word-moving-distance&amp;#12290;&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;想法很简单，就是去掉一个WMD的约束条件。假设去除第二个约束条件，原式变成
$$
\min_{T&amp;gt;=0}\sum_{i,j=1}^nT_{ij}c(i,j)　\\
subject \ to: \sum_{j=1}^nT_{i,j} = d_i, \forall_i \in \{1,...,n\} \tag{5}
$$
因为在所有满足WMD的解之中，肯定也可以找到一个满足RWMD的解，所以RWMD可以作为WMD的一个下限。从直观上理解，去掉约束条件后，文章B的词语并不一定都要被映射到，我们只要让每一个文章A的词语都旅行到文章B即可（举个极端的例子，文章A的所有词语都映射到文章B的某一个词）。类比于运输问题，即是我们只对产量有要求，对销量没有要求。在这种情况下，可想而知，我们只要让文章A的所有词都映射到离其词向量最近的点即可得到最优解。
$$
T_{ij}^* = \begin{cases}
d_{i}\ if j=argmin_{j}c(i,j) \\
0 \ otherwise.
\end{cases} \tag{6}
$$
可以得到
$$
\sum_jT_{ij}c(i,j) \geq \sum_jT_{ij}^*c(i,j) \tag{7}
$$
因为计算RWMD只需要找个离每个词语的最近的词，他的复杂度是$O(p^2)$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;&amp;#20116;&amp;#12289;Prefetch-and-prune&quot;&gt;&amp;#20116;&amp;#12289;Prefetch and prune&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20116;&amp;#12289;Prefetch-and-prune&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;作者利用上述WCD和RWMD提出了一种快速查找某文章的k nearest neighbors的算法。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算所有文章对该文章的WCD，然后按升序排序&lt;/li&gt;
&lt;li&gt;计算前k个文章的WMD（计算k nearest neighbors的WMD）&lt;/li&gt;
&lt;li&gt;然后来计算剩余文章的RWMD，如果某文章的RWMD大于第k个文章的WMD，那么就不再考虑该文章。反之，我们更新k nearest neighbors&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;&amp;#20845;&amp;#12289;&amp;#32467;&amp;#26524;&quot;&gt;&amp;#20845;&amp;#12289;&amp;#32467;&amp;#26524;&lt;a class=&quot;anchor-link&quot; href=&quot;#&amp;#20845;&amp;#12289;&amp;#32467;&amp;#26524;&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;通过以下结果可知，这种计算文章距离的算法击败了当时大部分的SOAT。
&lt;img src=&quot;/DataScienceBlog/images/copied_from_nb/images/WMD2.png&quot; alt=&quot;WDM2&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-&amp;#24605;&amp;#32771;&quot;&gt;3. &amp;#24605;&amp;#32771;&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-&amp;#24605;&amp;#32771;&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;在有了word2vec后，计算文章距离的想法其实很容易就能够想到。不过如果没有近似算法，那么就没有实际利用价值。做科研，理论和实践两手都要硬啊。&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. &lt;a href=&quot;http://proceedings.mlr.press/v37/kusnerb15.pdf&quot;&gt;From Word Embeddings To Document Distances&lt;/a&gt;&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. &lt;a href=&quot;https://baike.baidu.com/item/%E8%BF%90%E8%BE%93%E9%97%AE%E9%A2%98/12734790?fr=aladdin&quot;&gt;百度百科：运输问题&lt;/a&gt;&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Bujie Xu</name></author><summary type="html"></summary></entry></feed>