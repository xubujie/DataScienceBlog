{
  
    
        "post0": {
            "title": "基于类别的情感分析系统搭建",
            "content": "1. &#39033;&#30446;&#32972;&#26223;&#21644;&#38382;&#39064;&#25551;&#36848; . 维基百科的定义里说，文本情感分析是指用自然语言处理、文本挖掘以及计算机语言学等方法来识别和提取原素材中的主观信息。情感分析的目的是找出作者对某个事物的观点。很多人在网上购物或者在外就餐之后，都会通过发表评论来描述自己的体验。对于商家来说，分析这些文本就可以较好的看清楚用户的反馈，从而提升服务质量。另外，在投资领域，情感分析也有所应用。通过分析twitter上对某家公司的整体情感，可以给出大众对这家公司的一个整体看法，从而对投资起到一个指导作用。 . 本文将介绍怎么样从0开始搭建一个简单的情感分析系统。所用的到的数据来自Yelp Dataset, Yelp对标国内的大众点评，这个数据集里收集了很多的用户评论信息。下图是来自yelp网站的一个例子。 . . 在Review Highlights里，我们可以看到一些用户评价，而且Yelp还对评价进行了归类，比如第一类是有对souvenirs的评价，第二类是对electronics的评价。这里的souvenirs和electronics我们可以看作是商品或者是服务的一个维度(Aspect)。我们要搭建的情感分析系统要在传统的情感分析系统上做一点提高。 . 我们不断要判断用户评价是否是积极的，而且我们要从用户评价中获取用户是对哪个商品维度所进行的描述，并判断用户对每一个商品维度的情感。 对于每一条评价，我们想通过系统获取如下结果。 . { Business Name: XXXXX, Overall Rating: X, aspect1: { rating: XXX, pos: [XXX], neg: [XXX]} aspect2: {rating: XXX, pos: [XXX], neg: [XXX]} aspect3: {rating: XXX, pos: [XXX], neg: [XXX]} …… aspect5: {rating: XXX, pos:[xxx], neg:[xxxx]} } . 2. baseline&#25645;&#24314; . 在这里，我们用到yelp数据集里的“yelp_academic_dataset_business”和“yelp_academic_dataset_review”。可以通过这个链接查看样本数据。 . 我们的baseline包含以下几个部分 . 获取每个business的overall Rating | 获取每一个business的aspect | 获取对于aspect的评价 对于每个aspect的综合评价 | 从用户评论中获得对于某个aspect的相关内容，并进行分类（判断是好评还是差评） | | &#33719;&#21462;Overall Rating . 我们只需要对每个business的所有评论的结果求平均即可得到Overall Rating . for Id in business_id: star = [] for review in reviews[Id]: star.append(review[&#39;stars&#39;]) result[Id] = star.mean() . &#33719;&#21462;&#27599;&#19968;&#20010;business&#30340;aspect . 提取aspect，我们采用最简单的方法，对于每一个business，循环所有评论，统计评论中的名字词频，在去除stopwords之后，找到排名前5的词语，即作为该business的top 5 aspects。 . # 从每个评论中提取名词 def extract_and_add_aspects(aspects_dict, review): business_id = review[&#39;business_id&#39;] text = review[&#39;text&#39;].lower() text = word_tokenize(text) aspects = [tag[0] for tag in nltk.pos_tag(text,tagset=&#39;universal&#39;) if tag[1] == &#39;NOUN&#39;] aspects_dict[business_id].append(aspects) # 对于每一个business，提取在评论中词频最高的5个名词（去除stopword之后） def get_top_5_aspects(aspects_dict, business_id): l = list(itertools.chain.from_iterable(aspects_dict[business_id])) sorted_l = Counter(l).most_common() top_5_aspects = [] for s in sorted_l: if len(top_5_aspects) == 5: break elif s[0] in stopwords.words(&#39;english&#39;): continue else: top_5_aspects.append(s[0]) return top_5_aspects . &#33719;&#21462;&#23545;&#20110;aspect&#30340;&#35780;&#20215; . 为了获取对每一个aspect的评价，我们首先需要训练一个情感分析的模型。由于是baseline，我们首先尝试一个简单的模型来建立这个情感分析模型。本文采用tfidf vectorizer+random forest来建模。详细步骤分为以下几步 . 对所有的review中的文本（text）进行tfidf变换 | 将tfidf生成的矩阵作为输入，review的stars作为输出进行模型的训练。 | 用random forest来模型进行训练。因为这里的stars的取值为[0,1,2,3,4,5],　是一个有序的分类问题，我们也可以把该问题看成是一个回归问题，用regressor的效果会更好 | from sklearn.ensemble import RandomForestRegressor from sklearn.feature_extraction.text import TfidfVectorizer tidif_vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, max_features=5000) review_tidif_vector = tidif_vectorizer.fit_transform(df_review[&#39;text&#39;]) clf.fit(review_tidif_vector, df_review[&#39;stars&#39;]) . 训练完模型之后，我们还需要做关键的另一步，就是从用户评论中获取跟对应aspect相关的部分。这里，我们采取最简单的方法，首先按标点符号(这里我们只用了&#39;,.!&#39;)将用户评价进行切分。然后对于每个aspect，如果分句中出现了他，则判断该分局与这个aspect有关。 . def get_segment(aspect, text): &#39;&#39;&#39; Input an aspect and a text, the function will return the setences contain the aspect, if there are no such aspec the segment could be empty list. &#39;&#39;&#39; setences = re.split(&#39;[,.!]&#39;, text) segment = [] for s in setences: if aspect in s: segment.append(s.strip()) return segment . 完成了上两步后，我们既可以对每一个business的每一个review进行分析，求得最终结果。 . for review in business_id.get_reviews(): if review.contains(aspect1): review_segment = get_segment(review, aspect) score = sentiment_model.predict_prob(review_segment) if score &gt; threshold: pos_sent[review] = score else: neg_sent[review] = score . 在文本数据的预处理方面，常用的还有count vectorizer和word embedding， count vectorizer的效果比起tfidf略微差了一点，word embedding对结果会有所提高。另外，在情感分析模型的构建中，还可以有很多其他的方法，例如目前最流行的Bert。或者也可以利用LSTM来构建模型。在这方面，如果有兴趣可以参考https://www.kaggle.com/poonaml/bidirectional-lstm-spacy-on-yelp-reviews。 结果会比baseline有很大的提升。不过如果想要尝试这类深度学习的方法，必须先准备好GPU。 . 对于Aspect的抽取，Segment的抽取。本文的方法比较基础，这是考虑了最基础的情况。这些方面的抽取也可以考虑通过训练模型来实现。 . 4. &#32467;&#26524; . 以下给出一个对于某个business的结果精选。 . ========= sample result for business_id jRfQX8enRhWHf7V5zP5U8g ========= . Business Name: The Gelato Spot . Overall Rating: 4.058315334773218 . Aspects1 :gelato, rating:3.7201255688450567, . pos_sample: [&#39;This review is only with respect to their gelato&#39;, &quot;We haven&#39;t yet ventured beyond gelato but will make it our next stop when we&#39;re in the mood for pizza&quot;, &#39;The gelato LOOKS delicious from the get-go&#39;, &#39;The guy behind the gelato counter was very helpful and friendly&#39;, &#39;I saw a decent amount of customer flow and the employees were constantly making gelato in the back&#39;], . neg_sample: [&quot;Worst gelato I&#39;ve had&quot;, &#39;The girls serving gelato had horrible attitudes and kept pushing me off to each other (they openly said they did not want to wait on me and told each other to just have me wait and to help someone else)&#39;, &#39;their gelato was nothing special&#39;] . Aspects2 :pizza, rating:3.6547648329047857, . pos_sample: [&quot;We haven&#39;t yet ventured beyond gelato but will make it our next stop when we&#39;re in the mood for pizza&quot;, &quot;one of these days I&#39;ll try the pizza place as well and I&#39;m sure it won&#39;t disappoint&quot;, &#39;Vegetarian Friendly Entree: Honey Basil Pizza nDelicious pizza with the sweet and savory combination&#39;, &#39;One pizza is a perfect serving for one person&#39;, &quot;I&#39;m the type of person that can never choose the type of pizza I want&quot;], . neg_sample: [&#39;Showed up Saturday night to order a pizza and was told I could order but had to take it to go since they were closing in 30 minutes&#39;, &#39;Was told it took 15 minutes for the pizza to be ready&#39;, &#39;Worst pizza I have ever had&#39;, &#39;Lean cuisine pizzas have more flavor&#39;, &quot;the margherite pizza was the worst I&#39;ve had&quot;] . Aspects3 :place, rating:3.671313509696845, . pos_sample: [&#39;The place is very clean and nicely decorated&#39;, &#39;This place is great&#39;, &#39;The place is conveniently located in a corner location within a pizzeria&#39;, &#39;The location is clean and the place is quaint and comfortable&#39;, &#39;I frequent this place more often than I care to admit but with free samples and a great variety of different gelato flavors how could I not? Some common reasons why you might find me in Gelato Spot: n n- Holiday n- Bad Day n- Good Day n- Normal Day n- Sunday&#39;], neg_sample: [&#39;I decided to order pick up using the Eat2 services for lunch since my work place is walking distance from the restaurant&#39;, &#39;After helping them I told her I was there to pick up an order i placed using the Yelp app&#39;, &#39;I showed her the charge from my bank account and she placed the order to the cooks&#39;, &#39;I had to rush back to my work place since I only have a 30min lunch&#39;] . Aspects4 :flavors, rating:3.6733356958870567, . pos_sample: [&#39;Great flavors and good quality&#39;, &#39;Their flavors are all delicious&#39;, &#39;and they are great at suggesting new flavors&#39;, &#39;and they have so many flavors to choose from&#39;, &quot;We haven&#39;t been disappointed in any flavors that we&#39;ve tried: tiramisu&quot;], . neg_sample: [] . Aspects5 :spot, rating:3.6346406123098576, . pos_sample: [&#39;I was pleasantly surprised with this spot&#39;, &#39;It hit the spot&#39;, &#39;I am so happy that we found this spot and if I ever come back&#39;, &#39;This review is for Wood Fired pizza which is connected to the gelato spot&#39;, &#39;The G spot has it all&#39;], . neg_sample: [&#39;My daughter read about their cakes on their website so when I asked her where I should get her birthday cake she told me that Gelato spot had cakes with buttercream frosting(her favorite)&#39;] . 4. &#20854;&#20182;&#23581;&#35797; . 搭建一个情感分析系统需要牵扯到很多方面，任何一个环节都需要注意。如果有一个环节出错，都会影响到整个系统的表现。例如Aspect的抽取，Segment的抽取，或者是模型的构建。 . 1. 文本情感分析↩ . 2. Bidirectional LSTM, SpaCy on Yelp Reviews↩ . 3. Yelp Dataset↩ .",
            "url": "https://xubujie.github.io/DataScienceBlog/nlp/2020/08/07/Sentiment-Analysis-System.html",
            "relUrl": "/nlp/2020/08/07/Sentiment-Analysis-System.html",
            "date": " • Aug 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Sequenc to Sequence Learning with Nueral Networks",
            "content": "这篇文章发表于2014年底。我们所熟知的seq2seq模型由该文章提出。本文提出了一种解决由一个序列预测另一个序列问题的端到端的模型(seq2seq模型)。通过在WMT&#39;14数据集上的测试，证明了端到端的seq2seq模型可以达到SOAT的精度。 . 接下来我们主要介绍以下几点： . 背景和模型概要 | 实验设计和工程实现 | 实验结果 | 总结 | . 1. &#32972;&#26223;&#21644;&#27169;&#22411;&#27010;&#35201; . 传统的DNN虽然在很多领域取得了很好的成绩，但是DNN之只能适用于输入和输出的维度都确定的情况下。对于输入和输出长度不固定的问题，传统的DNN束手无策。 在本篇文章，作者为了解决此问题，提出了一种seq2seq的模型来处理。 . . 如上图所示，作者利用一个LSTM来编码输入序列，然后用另一个LSTM来对输出进行解码。这里用&quot;EOS&quot;来表示句尾，从而使得输出的序列长度由模型来决定。 . 2. &#23454;&#39564;&#35774;&#35745;&#21644;&#24037;&#31243;&#23454;&#29616; . 2.1 &#25968;&#25454;&#21644;&#35780;&#20215;&#25351;&#26631; . 实验所采用的数据集来自WMT&#39;14里英文到法文的翻译任务的数据。总共句子数为12M，其中法文单词有348M，英文单词有304M。另外作者事先给定了两种语言的词库，英文词库包含160,000常用词，法文词库包含80,000常用词，其他单词用&quot;UNK&quot;表示。 . BLEU来评价机器翻译的好坏。BLEU公式如下 . $$ BLEU = BP times e^{ sum_{i=1}^nw_i log(p_i)} $$$$ BP = begin{cases} 1 &amp; text{if c&gt;r} e^{1-r/c} &amp;c leq r end{cases} $$这里r为一个参考长度，当翻译结果长度为r时，不需要对结果进行惩罚 . 2.2 &#35757;&#32451;&#35814;&#24773; . 模型由4层LSTM作为编码器，另外一个4层LSTM作为解码器。输入的词向量维度为1000。其他实验中的细节如下 . 用一个范围为-0.08到0.08的均一分布来初始化LSTM的参数 | 用SGD进行训练，初始5个epoch的学习率设为0.7，之后每半个epoch学习率减半，总共训练了7.5个epoch | batch size为128， 每个batch里面的句子长度尽量相近 | 为了防止梯度爆炸，采取了梯度裁剪 | . 此外，作者还提到了一个特别重要的技巧，就是将输入进行倒排。通过实验，发现这样可以大大提升结果。从直观分析来说，这个技巧能起到作用的原因是将输出的词和输出的词在神经网络上的距离拉近了。例如在figure1里面，原来A到X的距离为4个单元，通过倒排A到X的距离变为2个单元。 . 在工程实现上，作者采用了8个GPU，每一层的LSTM用一个GPU计算，在前一个GPU计算完结果后会传递到下一个GPU来计算。另外4个GPU则用于softmax的并行计算。 . 3. &#23454;&#39564;&#32467;&#26524; . 实验结果如下面的table2所示，可以看出端到端的Seq2Seq模型在翻译任务上取到了比较不错的结果。提高Beam search的size和增加集成模型的各种都有助于提高模型精度。 . . 另外，作者还分析了编码器对于语序和语态的表现能力。从图中可以看出模型对于语序非常敏感，但是不能很好的区分主动语态和被动语态。 . . 最后作者比较了LSTM的seq2seq模型和baseline(统计模型)在各种句子长度下的表现，发现LSTM在各种句子长度下都能有比较好的效果。 另外figure3的右图指出，在句子里常用词比较多的情况下，LSTM的表现会更好。 . . 4. &#24635;&#32467; . 本文提出了一种处理不固定长序列问题的端到端的方法。大大简化了以往统计建模的时间，并且精度上也达到了一个不错的级别。个人感觉，在建模上，大家可以都能想到seq2seq这种结构。但是一些trick(比如倒排输入的句子)不太容易想到，而且在工程实现上，利用多个GPU并行计算LSTM在当时也不容易做到 . 1. Sequence to Sequence Learning with Neural Networks↩ . 2. 自然语言处理——BLEU详解以及简单的代码实现↩ .",
            "url": "https://xubujie.github.io/DataScienceBlog/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks.html",
            "relUrl": "/nlp/2020/07/31/Sequence-to-Sequence-Learning-with-Nueral-Networks.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Evaluation methods for unsupervised word embeddings",
            "content": "本文比较了各种衡量词向量的方法，并提出了一种新的评测词向量的方法。本文主要有以下贡献 . 分析了不同评判标准间的关系，表明了生成词向量的方式要和特殊任务相关联 | 提出了一种通过人为评分方式衡量直接衡量单个词向量的方法 | 提出了选择词向量（用于评价）时要考虑到选择不同词频，词性，词义的向量。保证数据的多样性 | 本文还发现了词向量包含着词频信息 | . 需要注意的是这篇文章的目的不是去比较词向量的好坏，而是去研究评判词向量方法的差别。 . 1. Embeeding&#30340;&#20934;&#22791; . 本文准备了以下六种生成词向量的方式用于评判： . 基于概率预测的embeeding CBOW model of word2vec (Mikolov et al 2013a) | C&amp;W embeddings (Collobert et al. 2011) | . | 基于反应语料中的词汇的同现关系 Hellinger PCA (Lebret and COllobert 2014) | GloVe (Pennington et al., 2014) | TSCCA (Dhillon et al., 2012) | Sparse Random Projections (Li et al., 2006) | . | . 对于C&amp;W的词向，因为只有基于2007年的维基百科的。所以本文选取了2008-03-01日的维基百科来训练其余5中词向量。这里，所有词向量的维度为50，总共的词典大小为103647 . 2. Evaluation . 评价词向量主要有两种方式，一种是内部评价（intrinsic evaluation），另一种是外部评价（extrinsic evaluation）。 . 内部评价指的是用词的词性，相关性等内部固有关系来评价生成的词向量的好坏。 外部评价指的是用生成的词向量去作为下游任务的输入，看哪种词向量可以更好的实现下游任务。 . &#20869;&#37096;&#35780;&#20215;&#65288;intrinsic evaluation&#65289; . 对于内部评价，本文采用的绝对的内部评价（absolute intrinsic evaluation）和相对的内部评价（comparative intrinsic evaluation），绝对内部评价有以下方法 . Relatedness：比较生成的词向量的词于词之间的余弦相似度和人类评价的相似度的关系 | Analogy：对于一个y，去找到一个x，使得x:y的关系要和a:b的关系一样 | Categorization：把生成的词向量做聚类，看聚类是否准确 | Selectional preference：确定一个词是某个动词的主语还是宾语 | . 评价结果如下,可以看出，绝大多数任务中，CBOW表现最好。但是个别任务里，其他词向量更好 . . 在相对内部评价中，用户直接来判断词向量的好坏。作者的具体做法如下， . 选取了词频，词性和词义不同的100个单词（选择10种类别的词，每种类别里有一个形容词，一个动词，4个名词，4个动词） | 找出每个词的n nearest neighbors, 选取rank为1，5，50的neighbor。所以对于6中词向量，对于每一个词，我们分为计算出rank为1，5，50的neighbor。 | 让人类来分别评价6中词向量中，rank1，5，50的neighbor里哪个于选定词最近。 | . 结果如下,同样可以看出，没有一种词向量是在所有任务中都表现最好的 . . 在相似度（relatedness）的比较中，我们对于任意一个单词，我们只找了一个相近的单词，这并不理想（因为每个单词都有很多近义词）。所以作者提出了一种新的衡量方式：Coherence。对于每一个单词，事先选出两个近义词和一个不相关的词，看用生成的词向量能否辨别出不想关的词。 . 结果如下,可以看出不同词向量的生成方法，对于不同词频的单词，所得到的结果是不同的 . . &#22806;&#37096;&#35780;&#20215;&#65288;Extrinsic evaluation&#65289; . 外部评价主要用来测量词向量对于下游任务的贡献。本文选取了以下两种下游任务来评判 . Noun phrase chunking：名词分块 | Sentiment classification：情感分类 | . 结果如下，对于下游任务，同样的，没有一种词向量可以在所有下游任务中都表现最好，所以对于不同下游任务，我们应该尝试不同词向量的表示 . . 3. Frequency information . 最后，作者通过以下两种实验发现了词向量里面包含词频信息。 . 用词向量来预测单词在语料中词频 | 对于所有在WordSim-353数据集的单词，研究其K=1000 nearest neighbors和他们在语料中词频的大小排序。 | . 结果如下,可以看出，我们可以通过词向量来较好的预测单词的词频，其中GloVe和CCA中包含了较多的词频信息。另外单词的词频于其在语料库里的词频排名也有很强的相关性 . . 4. &#24605;&#32771; . 通过本文，我们发现没有任何一种词向量可以在所有任务中都表现的最好，所以每个单词应该不存在一种绝对正确的词向量。那么，词向量是否是用来表示单词的最好方式呢，我对此表示疑问。以后很有可能会发现一种新的表示单词的方式。 .",
            "url": "https://xubujie.github.io/DataScienceBlog/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings.html",
            "relUrl": "/nlp/2020/07/04/Evaluation-methods-for-unsupervised-word-embeddings.html",
            "date": " • Jul 4, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "文本预处理方法汇总",
            "content": "本文汇总各种文本预处理的方法，皆在方便自己快速查找。 original link is here https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908 . all to upper case or lowwer case . input_str = &quot;AbcdEfG&quot; input_str.lower() . &#39;abcdefg&#39; . replace numbers or remove numbers . import re input_str = &#39;Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.&#39; result = re.sub(r&#39; d+&#39;, &#39;&#39;, input_str) print(result) . Box A contains red and white balls, while Box B contains red and blue balls. . Remove Punctuation . import string input_str = &quot;This &amp;is [an] example? {of} string. with.? punctuation!!!!&quot; punctuation_dict = {ord(p):&#39;&#39; for p in string.punctuation} result = input_str.translate(punctuation_dict) print(result) . This is an example of string with punctuation . Remove Whitespace . input_str = &quot; This has a lot whitespace &quot; print(input_str.strip()) . This has a lot whitespace . Tokenization . . from nltk.tokenize import WhitespaceTokenizer tokenizer = WhitespaceTokenizer() s = &quot;I love you&quot; tokenizer.tokenize(s) . [&#39;I&#39;, &#39;love&#39;, &#39;you&#39;] . Remove Stop words . from nltk.corpus import stopwords stop_words = stopwords.words(&#39;english&#39;) s = &quot;NLTK is a leading platform for building Python programs to work with human language data.&quot; result = [w for w in s.split() if w not in stop_words] print(result) . [&#39;NLTK&#39;, &#39;leading&#39;, &#39;platform&#39;, &#39;building&#39;, &#39;Python&#39;, &#39;programs&#39;, &#39;work&#39;, &#39;human&#39;, &#39;language&#39;, &#39;data.&#39;] . from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS print(ENGLISH_STOP_WORDS) . frozenset({&#39;together&#39;, &#39;seemed&#39;, &#39;she&#39;, &#39;hers&#39;, &#39;ie&#39;, &#39;may&#39;, &#39;becoming&#39;, &#39;though&#39;, &#39;everything&#39;, &#39;only&#39;, &#39;somewhere&#39;, &#39;at&#39;, &#39;one&#39;, &#39;very&#39;, &#39;few&#39;, &#39;many&#39;, &#39;whither&#39;, &#39;my&#39;, &#39;onto&#39;, &#39;now&#39;, &#39;keep&#39;, &#39;mill&#39;, &#39;this&#39;, &#39;than&#39;, &#39;once&#39;, &#39;seems&#39;, &#39;might&#39;, &#39;please&#39;, &#39;these&#39;, &#39;among&#39;, &#39;hence&#39;, &#39;thus&#39;, &#39;something&#39;, &#39;rather&#39;, &#39;how&#39;, &#39;whereas&#39;, &#39;whence&#39;, &#39;everywhere&#39;, &#39;last&#39;, &#39;anyone&#39;, &#39;never&#39;, &#39;somehow&#39;, &#39;another&#39;, &#39;herself&#39;, &#39;i&#39;, &#39;detail&#39;, &#39;two&#39;, &#39;elsewhere&#39;, &#39;give&#39;, &#39;nowhere&#39;, &#39;myself&#39;, &#39;me&#39;, &#39;some&#39;, &#39;of&#39;, &#39;everyone&#39;, &#39;first&#39;, &#39;yourselves&#39;, &#39;himself&#39;, &#39;meanwhile&#39;, &#39;serious&#39;, &#39;found&#39;, &#39;hereafter&#39;, &#39;much&#39;, &#39;becomes&#39;, &#39;nobody&#39;, &#39;thin&#39;, &#39;namely&#39;, &#39;find&#39;, &#39;indeed&#39;, &#39;thru&#39;, &#39;those&#39;, &#39;no&#39;, &#39;noone&#39;, &#39;both&#39;, &#39;is&#39;, &#39;hasnt&#39;, &#39;own&#39;, &#39;not&#39;, &#39;amoungst&#39;, &#39;empty&#39;, &#39;then&#39;, &#39;their&#39;, &#39;again&#39;, &#39;further&#39;, &#39;itself&#39;, &#39;most&#39;, &#39;hereby&#39;, &#39;up&#39;, &#39;wherein&#39;, &#39;to&#39;, &#39;thereupon&#39;, &#39;across&#39;, &#39;on&#39;, &#39;along&#39;, &#39;except&#39;, &#39;done&#39;, &#39;anyway&#39;, &#39;had&#39;, &#39;go&#39;, &#39;any&#39;, &#39;will&#39;, &#39;often&#39;, &#39;upon&#39;, &#39;three&#39;, &#39;fire&#39;, &#39;neither&#39;, &#39;anyhow&#39;, &#39;either&#39;, &#39;there&#39;, &#39;forty&#39;, &#39;re&#39;, &#39;per&#39;, &#39;formerly&#39;, &#39;beside&#39;, &#39;seeming&#39;, &#39;inc&#39;, &#39;amount&#39;, &#39;un&#39;, &#39;could&#39;, &#39;out&#39;, &#39;against&#39;, &#39;twelve&#39;, &#39;system&#39;, &#39;mostly&#39;, &#39;down&#39;, &#39;other&#39;, &#39;between&#39;, &#39;thereafter&#39;, &#39;below&#39;, &#39;full&#39;, &#39;our&#39;, &#39;would&#39;, &#39;anything&#39;, &#39;are&#39;, &#39;almost&#39;, &#39;but&#39;, &#39;bottom&#39;, &#39;your&#39;, &#39;made&#39;, &#39;see&#39;, &#39;until&#39;, &#39;eg&#39;, &#39;beforehand&#39;, &#39;as&#39;, &#39;therefore&#39;, &#39;cannot&#39;, &#39;enough&#39;, &#39;what&#39;, &#39;became&#39;, &#39;con&#39;, &#39;through&#39;, &#39;front&#39;, &#39;six&#39;, &#39;from&#39;, &#39;all&#39;, &#39;the&#39;, &#39;put&#39;, &#39;someone&#39;, &#39;throughout&#39;, &#39;former&#39;, &#39;has&#39;, &#39;still&#39;, &#39;due&#39;, &#39;next&#39;, &#39;fifteen&#39;, &#39;off&#39;, &#39;and&#39;, &#39;cant&#39;, &#39;alone&#39;, &#39;amongst&#39;, &#39;besides&#39;, &#39;side&#39;, &#39;about&#39;, &#39;we&#39;, &#39;he&#39;, &#39;eleven&#39;, &#39;always&#39;, &#39;was&#39;, &#39;whatever&#39;, &#39;none&#39;, &#39;whenever&#39;, &#39;whole&#39;, &#39;where&#39;, &#39;her&#39;, &#39;above&#39;, &#39;also&#39;, &#39;ourselves&#39;, &#39;four&#39;, &#39;top&#39;, &#39;fill&#39;, &#39;although&#39;, &#39;which&#39;, &#39;move&#39;, &#39;sixty&#39;, &#39;thence&#39;, &#39;were&#39;, &#39;nothing&#39;, &#39;bill&#39;, &#39;however&#39;, &#39;you&#39;, &#39;hundred&#39;, &#39;same&#39;, &#39;must&#39;, &#39;ltd&#39;, &#39;been&#39;, &#39;they&#39;, &#39;whereupon&#39;, &#39;sincere&#39;, &#39;name&#39;, &#39;because&#39;, &#39;hereupon&#39;, &#39;others&#39;, &#39;who&#39;, &#39;cry&#39;, &#39;his&#39;, &#39;while&#39;, &#39;if&#39;, &#39;too&#39;, &#39;since&#39;, &#39;or&#39;, &#39;sometimes&#39;, &#39;therein&#39;, &#39;without&#39;, &#39;ten&#39;, &#39;eight&#39;, &#39;via&#39;, &#39;five&#39;, &#39;into&#39;, &#39;ours&#39;, &#39;co&#39;, &#39;yours&#39;, &#39;an&#39;, &#39;themselves&#39;, &#39;us&#39;, &#39;for&#39;, &#39;thick&#39;, &#39;thereby&#39;, &#39;so&#39;, &#39;more&#39;, &#39;under&#39;, &#39;am&#39;, &#39;by&#39;, &#39;less&#39;, &#39;ever&#39;, &#39;otherwise&#39;, &#39;whoever&#39;, &#39;nine&#39;, &#39;even&#39;, &#39;wherever&#39;, &#39;a&#39;, &#39;yourself&#39;, &#39;herein&#39;, &#39;every&#39;, &#39;part&#39;, &#39;each&#39;, &#39;already&#39;, &#39;such&#39;, &#39;in&#39;, &#39;afterwards&#39;, &#39;be&#39;, &#39;least&#39;, &#39;why&#39;, &#39;anywhere&#39;, &#39;with&#39;, &#39;them&#39;, &#39;perhaps&#39;, &#39;latter&#39;, &#39;seem&#39;, &#39;back&#39;, &#39;during&#39;, &#39;can&#39;, &#39;else&#39;, &#39;being&#39;, &#39;over&#39;, &#39;whose&#39;, &#39;within&#39;, &#39;moreover&#39;, &#39;whereby&#39;, &#39;fifty&#39;, &#39;mine&#39;, &#39;several&#39;, &#39;get&#39;, &#39;its&#39;, &#39;well&#39;, &#39;take&#39;, &#39;whom&#39;, &#39;after&#39;, &#39;it&#39;, &#39;third&#39;, &#39;describe&#39;, &#39;whereafter&#39;, &#39;nor&#39;, &#39;that&#39;, &#39;before&#39;, &#39;interest&#39;, &#39;when&#39;, &#39;call&#39;, &#39;sometime&#39;, &#39;nevertheless&#39;, &#39;toward&#39;, &#39;show&#39;, &#39;latterly&#39;, &#39;twenty&#39;, &#39;yet&#39;, &#39;couldnt&#39;, &#39;have&#39;, &#39;him&#39;, &#39;around&#39;, &#39;become&#39;, &#39;do&#39;, &#39;behind&#39;, &#39;beyond&#39;, &#39;towards&#39;, &#39;here&#39;, &#39;de&#39;, &#39;should&#39;, &#39;whether&#39;, &#39;etc&#39;}) . Stemming &amp; Remove sparse terms and particular words . . from nltk.stem import PorterStemmer from nltk.tokenize import word_tokenize stemmer= PorterStemmer() input_str=&quot;There are several types of stemming algorithms.&quot; input_str=word_tokenize(input_str) for word in input_str: print(stemmer.stem(word)) . there are sever type of stem algorithm . . Lemmatization . Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core. . from nltk.stem import WordNetLemmatizer from nltk.tokenize import word_tokenize lemmatizer=WordNetLemmatizer() input_str=&quot;been had done languages cities mice&quot; input_str=word_tokenize(input_str) for word in input_str: print(lemmatizer.lemmatize(word)) . been had done language city mouse . Part of speech tagging (POS) . Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core. . from nltk import pos_tag from nltk import word_tokenize input_str=&quot;Parts of speech examples: an article, to write, interesting, easily, and, of&quot; tokens = word_tokenize(input_str) result = pos_tag(tokens) print(result) . [(&#39;Parts&#39;, &#39;NNS&#39;), (&#39;of&#39;, &#39;IN&#39;), (&#39;speech&#39;, &#39;NN&#39;), (&#39;examples&#39;, &#39;NNS&#39;), (&#39;:&#39;, &#39;:&#39;), (&#39;an&#39;, &#39;DT&#39;), (&#39;article&#39;, &#39;NN&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;write&#39;, &#39;VB&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;interesting&#39;, &#39;VBG&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;easily&#39;, &#39;RB&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;and&#39;, &#39;CC&#39;), (&#39;,&#39;, &#39;,&#39;), (&#39;of&#39;, &#39;IN&#39;)] . Chunking (shallow parsing) . Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) [23]. Chunking tools: NLTK, TreeTagger chunker, Apache OpenNLP, General Architecture for Text Engineering (GATE), FreeLing. .",
            "url": "https://xubujie.github.io/DataScienceBlog/nlp/2020/07/03/Text-Preprocessing.html",
            "relUrl": "/nlp/2020/07/03/Text-Preprocessing.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "From Word Embeddings To Document Distances详解",
            "content": "1. &#31616;&#20171; . 《From Word Embeddings To Document Distances》这篇文章，发表于2014年。作者在word2vec的基础上提出了一种衡量文章相似度的尺度，Word Mover&#39;s Distance(WDM)。 WDM用来描述两篇文章的词向量之间的距离。这里的“距离”作者用了“旅行距离”来描述，意思是从一篇文章的词向量转换到另一篇文章的词向量的最短距离。在此距离的基础上，作者提出了衡量文章相似度的算法。从实验可以看出，该方法优于当时的SOTA。 . 下文会按照作者的思路来详细介绍WDM的定义和文章相似度的计算方法（站在作者角度来想）： . 在计算文章相似度的问题中，前人的方法只考虑词频，没有考虑文章的意义，把word2vec用到文章相似度的评价里会不会有所提高呢？ | 那么我来定义了一种新的计算文章相似度的方法WMD。 | 可是WMD的计算复杂度太高。那我提出计算下限的方法WCD和RWMD。 | 有了WCD和RWMD，然后通过Prefetch和Prune来找到一个文章的k nearest neighbors。 | 看看我的实验结果多牛叉。超过了当时的SOTA。 | . 2. &#35770;&#25991;&#35814;&#35299; . &#19968;&#12289;Word2Vec Embedding . 在2013年，Mikolov et al提出了word2vec，这个在当时引起了很大反响。该研究用向量来表示每个单词，从数学上更好的表示了单词的意义。传统上表示文章，大多数都是用BOW或者TFIDF的手法，这些手法对于词的意义有欠考虑，所以在衡量文章相似度上，往往停留在单词一致性的表层上，文章意思的相似性得不到很大的表现。作者关注到word2vec的发展，然后将该方法应用到了文章相似度的表达上。这里我们首先介绍以下word2vec。 . 简单来说，word2vec是一种通过学习神经网络来寻找词向量表示的一种方法。具体来说word2vec的skip-gram模型是通过构建一个单层神经网络（input layer, projection layer, output layer)来预测每个单词的相邻单词。通过学习该神经网络，得到的projection layer即为每个单词的词向量。训练目标是使每个单词的相邻单词的出现概率最大化。该概率可以用下式表示 $$ frac{1}{T} sum_{t=1}^T sum_{j in nb(t)}logp(w_j|w_t) tag{1} $$ . 这里T为单词数，nb(t)表示单词$w_t$的相邻单词，$p(w_j|w_t)$用hierarchical softmax来提高训练速度。 . &#20108;&#12289;Word Move&#39;s Distance . 首先我们用词向量来定义Word travel cost。假设$x_i, x_j$表示word $i$和word $j$的词向量。我们用$c(i, j) = ||x_i - x_j||_2$来表示从一个词到另一个词的&quot;旅行距离&quot;（Word travel cost）。有了词于词之间的距离，接下来我们来定义文章间距离。设$d$, $d&#39;$为两篇文章的nBow (normalized bag of words)表示。令$T_{ij}, T in R^{n times n}$来表示文章d的词i到文章d&#39;的距离。我们令词i到文章d&#39;的所有词的距离之和为$ sum_jT_{ij} = d_i$, 反之文章d&#39;的词j到文章d的所有词之和为$ sum_iT_{ij}=d&#39;_j$。最终，我们定义两个文章的距离为从一篇文章d到另一篇文章d‘距离的加权累加的最小值。用数学描述为 $$ min_{T&gt;=0} sum_{i,j=1}^nT_{ij}c(i,j)　 subject to: sum_{j=1}^nT_{i,j} = d_i, forall_i in {1,...,n } sum_{i=1}^nT_{i,j} = d&#39;_j, forall_j in {1,...,n } tag{2} $$ 所以，求解文章与文章之间的WMD转化为了一个最优化问题。 . 通过下图简要举例介绍词数相同时文章间距离和词数不同时文章距离的情况。在上半部分，去除了（the, to ,in, a）等等stopwords之后，D0,D1,D2都是由4个不同的词构成的。所以，所有的词的$d_i=0.25$，这里的箭头表示的是$T_{ij}c(i,j)$, 由于词向量空间上，比起band，Obama离President更近，这里的分数也很好的反应了该结果。在图的下半部分，我们可以看出，当词数不同时，一个词可能会映射到多个相似的词。 . . &#19977;&#12289;&#31867;&#27604;&#36816;&#36755;&#38382;&#39064; . 运输问题的典型情况是研究单一品种物质的运输调度问题：设某种物品有m个产地$A_1，A_2，···，A_m$，各产地的产量分别是$a_1，a_2，···，a_m$,有n个销地$B_1，B_2，···，B_n$，各个销地的销量分别为$b_1，b_2，···，b_n$。假定从产地$A_i(i=1,2,···,m)$向销地$B_j(j=1,2,···,n)$运输单位物品的运价为$c_{ij}$，怎么调运这些物品才能使总运费最小？可以看出，文章的WMD计算和运输问题是完全对应的。这里产地$A_1，A_2，···，A_m$对应于文章A的单词，$a_1，a_2，···，a_m$为单词的词频。同理$B_1，B_2，···，B_n$为文章B的单词，$b_1，b_2，···，b_n$为对应的词频。运价$c_{ij}$对应于文章A的单词和文章B的单词间的欧拉距离。运输问题可以描述成以下线性规划问题。 . $$ min_z = sum_{i=1}^m sum_{j=1}^nc_{ij}x_{ij} subject to: sum_{j=1}^nx_{i,j} = a_i, forall_i in {1,...,n } sum_{i=1}^nx_{i,j} = b_j, forall_j in {1,...,n } x_{ij} geq 0 tag{3} $$ &#22235;&#12289;Fast Distance Computation . 解决上述WMD最优化问题的复杂度时$O(p^3 log p)$, p为去重后单词数。可以想象，当文章有很多单词时，计算WMD会变的非常困难。为此，作者提出了两种求解WMD下限的方法 . WCD&#65306;&#36890;&#36807;&#19977;&#35282;&#19981;&#31561;&#24335;$||x+y|| leq ||x|| + ||y||$&#65292;&#25105;&#20204;&#21487;&#20197;&#31616;&#21333;&#30340;&#24471;&#21040; . $$ sum_{i,j=1}^nT_{ij}c(i,j) = sum_{i,j=1}^nT_{ij}||x_i-x&#39;_j||_2 = sum_{i,j=1}||T_{ij}(x_i-x&#39;_j)||_2 geq || sum_{i,j=1}^nT_{ij}(x_i-x&#39;_j)||_2 = || sum_{i=1}^n( sum_{j=1}^nT_{ij})x_i- sum_{j=1}^n( sum_{j=1}^nT_{ij})x&#39;_j||_2 = || sum_{i=1}^nd_ix_i - sum_{j=1}^nd&#39;_jx&#39;_j||_2 tag{4} $$作者将这个距离称为Word Centroid Distance（WCD），可以看出计算这个WMD的下限WCD非常快，算法复杂度是O(dp), d为词向量的维度，p为去重后的单词数。在寻找某篇文章的k个最相似的文章时，WCD可以用于寻找有利的候补，从而提高算法效率。 . RWMD&#65306;&#34429;&#28982;WCD&#24456;&#23481;&#26131;&#35745;&#31639;&#65292;&#20294;&#26159;&#22240;&#20026;&#20182;&#32473;&#20986;&#30340;&#19979;&#38480;&#22826;&#20302;&#65292;&#20316;&#32773;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36924;&#36817;&#30495;&#23454;&#20540;&#30340;&#19979;&#38480;Relaxed word moving distance&#12290; . 想法很简单，就是去掉一个WMD的约束条件。假设去除第二个约束条件，原式变成 $$ min_{T&gt;=0} sum_{i,j=1}^nT_{ij}c(i,j)　 subject to: sum_{j=1}^nT_{i,j} = d_i, forall_i in {1,...,n } tag{5} $$ 因为在所有满足WMD的解之中，肯定也可以找到一个满足RWMD的解，所以RWMD可以作为WMD的一个下限。从直观上理解，去掉约束条件后，文章B的词语并不一定都要被映射到，我们只要让每一个文章A的词语都旅行到文章B即可（举个极端的例子，文章A的所有词语都映射到文章B的某一个词）。类比于运输问题，即是我们只对产量有要求，对销量没有要求。在这种情况下，可想而知，我们只要让文章A的所有词都映射到离其词向量最近的点即可得到最优解。 $$ T_{ij}^* = begin{cases} d_{i} if j=argmin_{j}c(i,j) 0 otherwise. end{cases} tag{6} $$ 可以得到 $$ sum_jT_{ij}c(i,j) geq sum_jT_{ij}^*c(i,j) tag{7} $$ 因为计算RWMD只需要找个离每个词语的最近的词，他的复杂度是$O(p^2)$ . &#20116;&#12289;Prefetch and prune . 作者利用上述WCD和RWMD提出了一种快速查找某文章的k nearest neighbors的算法。 . 计算所有文章对该文章的WCD，然后按升序排序 | 计算前k个文章的WMD（计算k nearest neighbors的WMD） | 然后来计算剩余文章的RWMD，如果某文章的RWMD大于第k个文章的WMD，那么就不再考虑该文章。反之，我们更新k nearest neighbors | &#20845;&#12289;&#32467;&#26524; . 通过以下结果可知，这种计算文章距离的算法击败了当时大部分的SOAT。 . 3. &#24605;&#32771; . 在有了word2vec后，计算文章距离的想法其实很容易就能够想到。不过如果没有近似算法，那么就没有实际利用价值。做科研，理论和实践两手都要硬啊。 . 1. From Word Embeddings To Document Distances↩ . 2. 百度百科：运输问题↩ .",
            "url": "https://xubujie.github.io/DataScienceBlog/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances.html",
            "relUrl": "/nlp/2020/06/11/From-Word-Embeddings-To-Document-Distances.html",
            "date": " • Jun 11, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "",
          "url": "https://xubujie.github.io/DataScienceBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://xubujie.github.io/DataScienceBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}