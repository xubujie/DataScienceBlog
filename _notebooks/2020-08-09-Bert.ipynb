{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding \n",
    "> Bert: 2018 google\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Bujie Xu\n",
    "- categories: [NLP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在2014年Google发表attention机制以来，attention得到了广泛的研究，2017年Google又在“Attention Is All You Need”这篇论文里提出了Transformer模型。在对序列问题的建模上，Transformer大有取代RNN地位的势头。随后，在2019年Bert横空出世，横扫几乎所有NLP任务的榜单，引爆了NLP的研究社区。让越来越多的人投入到了这个领域的研究上来了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文简要介绍提出BERT的那篇论文。将从以下几个方面展开：\n",
    "- 背景和相关研究\n",
    "- BERT的结构和任务\n",
    "- BERT在NLP任务中的应用\n",
    "- 总结和反思"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在阅读这篇文章之前，读者最好对Transformer的模型结构有所了解。可以参考[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)(这算是介绍Transformer最好的文章了)。如果想要了解Transformer的实现，可以阅读[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 背景和相关研究"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在很多NLP的任务中，使用预训练模型对结果都有很大的提高。例如，在NLP任务中使用预训练的词向量，能够很好的提高模型精度。但是传统的词向量表征存在一个致命的问题，就是他们无法有效的表达一词多义的情况。随后ELMo, GPT, BERT相继得到提出，他们都可以很好的解决一词多义的问题。Elmo用两层LSTM独立的训练了从左到右和从右到左的语言表示，最终得到下游任务的特征表示，对不同任务，他需要训练不同的特征表示。GPT使用了Transformer从左到右对语料进行训练。BERT采用了双向Transformer对语料进行了训练。BERT和GPT都可以直接用于下游任务，并通过fine-tune微调网络参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bert1](images/bert1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bert的结构和任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，BERT是由多层的双向Transformer构成。在结构上，他相较于GPT的创新就是引入了双向Transformer。另外，BERT还引入了Masked Language Model(MLM)和Next Sentence Prediction(NSP)两个任务来做模型的训练。以下分别对他们进行介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用双向Transformer，Bert里设计了一种mask机制，就是对输入进行mask操作，然后预测被mask的词。具体的mask操作如下,对于选中的单词\n",
    "- 80%的概率置换成[MASK]\n",
    "- 10%的概率换成一个随机的单词\n",
    "- 10%的概率保留原单词\n",
    "\n",
    "下面给出了这个任务的一个例子。\n",
    "\n",
    "`\n",
    "Input: the man went to the [MASK1] . he bought a [MASK2] of milk.\n",
    "Labels: [MASK1] = store; [MASK2] = gallon\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Sentence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在很多NLP的任务里，理解句子间的关系很重要。比如说QA任务，或者是NLI任务。所以，在BERT中作者设计了NSP这个任务，这个任务其实很简单，就是去判断一句话是否跟另一句话相连。下面给出了两个例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "Sentence A: the man went to the store .\n",
    "Sentence B: he bought a gallon of milk .\n",
    "Label: IsNextSentence\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "Sentence A: the man went to the store .\n",
    "Sentence B: penguins are flightless .\n",
    "Label: NotNextSentence\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过研究表明，NSP这个任务对于QA任务或者是NLI任务都有很大的帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Bert在NLP任务中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在训练完BERT后，只用稍微进行以下Fine-tune，即可运用于其他的下游任务。\n",
    "![Bert Pretrain](images/Bert2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "下图展示了如何在各个不同NLP任务中使用BERT。业界戏称此图为麻将图\n",
    "\n",
    "![Bert4](images/Bert4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "本篇论文的结果是基于$BERT_{BASE}$和$BERT_{LARGE}$的，他们的参数如下(L为Transformer的层数，H为hidden size, A为self-attention的头的个数:\n",
    "\n",
    "- $BERT_{BASE}$: L=12, H=768, A=12\n",
    "- $BERT_{LARGE}$: L=24, H=1024, A=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "BERT横扫了多少NLP的下游任务版单，在这里我们就不一一列举了。下表列出了BERT在GLUE这个Benchmark上的表现，可以看出它对当时的SOTA有很大的提升。另外在SQuAD, SWAG等任务或者数据集上，BERT也是轻松刷新了最好成绩。\n",
    "![](images/Bert_table1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 总结和反思"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT的提出对于NLP领域有着至关重要的影响，在这之后Transfer Learning在NLP领域也渐渐得到广泛的应用。正如作者所说，BERT的模型结构不复杂，但是在实用中非常强大。Google能够在AI领域屡屡发表重要文章，理论是一个方面，代码能力也非常重要。能想到BERT结构的人应该不少，但是能把它实现起来确实比较难。\n",
    "\n",
    "另外，虽然NSP用于训练BERT对于一些NLP的下游任务有帮助，不过另外一些任务，例如文本分类。可能NSP的帮助不是很大，这里应该有提升的空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{ '[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)' | fndetail: 1 }}\n",
    "{{ '[Attention Is All You Need](https://arxiv.org/abs/1706.03762)' | fndetail: 2 }}\n",
    "{{ '[BERT](https://github.com/google-research/bert)' | fndetail: 3 }}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}