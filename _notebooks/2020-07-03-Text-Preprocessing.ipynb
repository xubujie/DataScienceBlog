{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理方法汇总\n",
    ">\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Bujie Xu\n",
    "- categories: [NLP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文汇总各种文本预处理的方法，皆在方便自己快速查找。\n",
    "original link is here https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all to upper case or lowwer case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"AbcdEfG\"\n",
    "input_str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace numbers or remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "result = re.sub(r'\\d+', '', input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\"\n",
    "punctuation_dict = {ord(p):'' for p in string.punctuation}\n",
    "result = input_str.translate(punctuation_dict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Whitespace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This has a lot whitespace\n"
     ]
    }
   ],
   "source": [
    "input_str = \"   This has a lot whitespace    \"\n",
    "print(input_str.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tokenization](https://miro.medium.com/max/3220/1*ffMYw8aujrmyxfA55Zm3Jg.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'you']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "s = \"I love you\"\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "s = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "result = [w for w in s.split() if w not in stop_words]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'together', 'seemed', 'she', 'hers', 'ie', 'may', 'becoming', 'though', 'everything', 'only', 'somewhere', 'at', 'one', 'very', 'few', 'many', 'whither', 'my', 'onto', 'now', 'keep', 'mill', 'this', 'than', 'once', 'seems', 'might', 'please', 'these', 'among', 'hence', 'thus', 'something', 'rather', 'how', 'whereas', 'whence', 'everywhere', 'last', 'anyone', 'never', 'somehow', 'another', 'herself', 'i', 'detail', 'two', 'elsewhere', 'give', 'nowhere', 'myself', 'me', 'some', 'of', 'everyone', 'first', 'yourselves', 'himself', 'meanwhile', 'serious', 'found', 'hereafter', 'much', 'becomes', 'nobody', 'thin', 'namely', 'find', 'indeed', 'thru', 'those', 'no', 'noone', 'both', 'is', 'hasnt', 'own', 'not', 'amoungst', 'empty', 'then', 'their', 'again', 'further', 'itself', 'most', 'hereby', 'up', 'wherein', 'to', 'thereupon', 'across', 'on', 'along', 'except', 'done', 'anyway', 'had', 'go', 'any', 'will', 'often', 'upon', 'three', 'fire', 'neither', 'anyhow', 'either', 'there', 'forty', 're', 'per', 'formerly', 'beside', 'seeming', 'inc', 'amount', 'un', 'could', 'out', 'against', 'twelve', 'system', 'mostly', 'down', 'other', 'between', 'thereafter', 'below', 'full', 'our', 'would', 'anything', 'are', 'almost', 'but', 'bottom', 'your', 'made', 'see', 'until', 'eg', 'beforehand', 'as', 'therefore', 'cannot', 'enough', 'what', 'became', 'con', 'through', 'front', 'six', 'from', 'all', 'the', 'put', 'someone', 'throughout', 'former', 'has', 'still', 'due', 'next', 'fifteen', 'off', 'and', 'cant', 'alone', 'amongst', 'besides', 'side', 'about', 'we', 'he', 'eleven', 'always', 'was', 'whatever', 'none', 'whenever', 'whole', 'where', 'her', 'above', 'also', 'ourselves', 'four', 'top', 'fill', 'although', 'which', 'move', 'sixty', 'thence', 'were', 'nothing', 'bill', 'however', 'you', 'hundred', 'same', 'must', 'ltd', 'been', 'they', 'whereupon', 'sincere', 'name', 'because', 'hereupon', 'others', 'who', 'cry', 'his', 'while', 'if', 'too', 'since', 'or', 'sometimes', 'therein', 'without', 'ten', 'eight', 'via', 'five', 'into', 'ours', 'co', 'yours', 'an', 'themselves', 'us', 'for', 'thick', 'thereby', 'so', 'more', 'under', 'am', 'by', 'less', 'ever', 'otherwise', 'whoever', 'nine', 'even', 'wherever', 'a', 'yourself', 'herein', 'every', 'part', 'each', 'already', 'such', 'in', 'afterwards', 'be', 'least', 'why', 'anywhere', 'with', 'them', 'perhaps', 'latter', 'seem', 'back', 'during', 'can', 'else', 'being', 'over', 'whose', 'within', 'moreover', 'whereby', 'fifty', 'mine', 'several', 'get', 'its', 'well', 'take', 'whom', 'after', 'it', 'third', 'describe', 'whereafter', 'nor', 'that', 'before', 'interest', 'when', 'call', 'sometime', 'nevertheless', 'toward', 'show', 'latterly', 'twenty', 'yet', 'couldnt', 'have', 'him', 'around', 'become', 'do', 'behind', 'beyond', 'towards', 'here', 'de', 'should', 'whether', 'etc'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "print(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Remove sparse terms and particular words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stemming](https://miro.medium.com/max/3492/1*JpOXoNSFkZ0sjqPYT2U4cA.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "input_str=\"There are several types of stemming algorithms.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str=\"been had done languages cities mice\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), (':', ':'), ('an', 'DT'), ('article', 'NN'), (',', ','), ('to', 'TO'), ('write', 'VB'), (',', ','), ('interesting', 'VBG'), (',', ','), ('easily', 'RB'), (',', ','), ('and', 'CC'), (',', ','), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
    "tokens = word_tokenize(input_str)\n",
    "result = pos_tag(tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking (shallow parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) [23]. Chunking tools: NLTK, TreeTagger chunker, Apache OpenNLP, General Architecture for Text Engineering (GATE), FreeLing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
